{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"memsearch","text":"<p>OpenClaw's memory, everywhere.</p> <p>Inspired by OpenClaw's memory system, memsearch brings the same markdown-first architecture to a standalone library. Pluggable into any agent framework, backed by Milvus.</p>"},{"location":"#why-memsearch","title":"Why memsearch?","text":"<pre><code>$ cat /dev/philosophy\nMarkdown is the source of truth.\nSimple. Readable. Git-friendly. Zero vendor lock-in.\nThe vector store is just a derived index \u2014 rebuildable anytime.\n</code></pre> <ul> <li>OpenClaw's memory, everywhere -- markdown as the single source of truth</li> <li>Smart dedup -- SHA-256 content hashing means unchanged content is never re-embedded</li> <li>Live sync -- file watcher auto-indexes on changes, deletes stale chunks</li> <li>Memory compact -- LLM-powered summarization compresses old memories</li> <li>Ready-made Claude Code plugin -- a drop-in example of agent memory built on memsearch</li> </ul>"},{"location":"#what-is-memsearch","title":"What is memsearch?","text":"<p>Most memory systems treat the vector database as the source of truth. memsearch flips this around: your markdown files are the source of truth, and the vector store is just a derived index -- like a database index that can be dropped and rebuilt at any time.</p> <p>This means:</p> <ul> <li>Your data is always human-readable -- plain <code>.md</code> files you can open, edit, grep, and <code>git diff</code></li> <li>No vendor lock-in -- switch embedding providers or vector backends without losing anything</li> <li>Rebuild on demand -- corrupted index? Just re-run <code>memsearch index</code> and you are back in seconds</li> <li>Git-native -- version your knowledge base with standard git workflows</li> </ul> <p>memsearch scans your markdown directories, splits content into semantically meaningful chunks (by heading structure and paragraph boundaries), embeds them, and stores the vectors in Milvus. When you search, it finds the most relevant chunks by cosine similarity and returns them with full source attribution.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>$ pip install memsearch\n</code></pre> <p>Say you have a directory of daily markdown logs (the same layout used by OpenClaw):</p> <pre><code>memory/\n\u251c\u2500\u2500 MEMORY.md          # persistent facts &amp; decisions\n\u251c\u2500\u2500 2026-02-07.md      # daily log\n\u251c\u2500\u2500 2026-02-08.md\n\u2514\u2500\u2500 2026-02-09.md\n</code></pre> <p>Index it and search:</p> <pre><code>$ memsearch index ./memory/\nIndexed 38 chunks.\n\n$ memsearch search \"how to configure Redis?\"\n\n--- Result 1 (score: 0.0328) ---\nSource: memory/2026-02-08.md\nHeading: Infrastructure Decisions\nWe chose Redis for caching over Memcached. Config: host=localhost,\nport=6379, max_memory=256mb, eviction=allkeys-lru.\n\n--- Result 2 (score: 0.0315) ---\nSource: memory/2026-02-07.md\nHeading: Redis Setup Notes\nRedis config for production: enable AOF persistence, set maxmemory-policy\nto volatile-lfu, bind to 127.0.0.1 only...\n\n$ memsearch watch ./memory/\nIndexed 8 chunks.\nWatching 1 path(s) for changes... (Ctrl+C to stop)\nIndexed 2 chunks from memory/2026-02-09.md\n</code></pre> <p>The <code>watch</code> command monitors your files and auto-indexes changes in the background -- ideal for use alongside editors or agent processes that write to your knowledge base.</p>"},{"location":"#python-api","title":"Python API","text":"<p>The core workflow is three lines: create a <code>MemSearch</code> instance, index your files, and search.</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\n\nasync def main():\n    mem = MemSearch(paths=[\"./memory/\"])\n\n    # Index all markdown files (skips unchanged content automatically)\n    await mem.index()\n\n    # Semantic search -- returns ranked results with source attribution\n    results = await mem.search(\"how to configure Redis?\", top_k=5)\n    for r in results:\n        print(f\"[{r['score']:.2f}] {r['source']} -- {r['content'][:80]}\")\n\n    mem.close()\n\nasyncio.run(main())\n</code></pre> <p>See Getting Started for a complete walkthrough with agent memory loops.</p>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#personal-knowledge-base","title":"Personal Knowledge Base","text":"<p>Point memsearch at your notes directory and get instant semantic search across years of accumulated knowledge.</p> <pre><code>$ memsearch index ~/notes/\n$ memsearch search \"that article about distributed consensus\"\n</code></pre>"},{"location":"#agent-memory","title":"Agent Memory","text":"<p>Give your AI agent persistent, searchable memory. The agent writes observations to markdown files; memsearch indexes them and retrieves relevant context on the next turn. This is exactly how OpenClaw manages memory, and memsearch ships with a ready-made Claude Code plugin that demonstrates the pattern.</p> <pre><code>mem = MemSearch(paths=[\"./agent-memory/\"])\n\n# Agent recalls relevant past experiences before responding\nmemories = await mem.search(user_question, top_k=3)\n\n# Agent saves new knowledge after responding\nsave_to_markdown(\"./agent-memory/\", today, summary)\nawait mem.index()\n</code></pre>"},{"location":"#team-knowledge-sharing","title":"Team Knowledge Sharing","text":"<p>Deploy a shared Milvus server and point multiple team members (or agents) at it. Everyone indexes their own markdown files into the same collection, creating a shared searchable knowledge base.</p> <pre><code>mem = MemSearch(\n    paths=[\"./docs/\"],\n    milvus_uri=\"http://milvus.internal:19530\",\n    milvus_token=\"root:Milvus\",\n)\n</code></pre>"},{"location":"#embedding-providers","title":"Embedding Providers","text":"<p>memsearch supports 5 embedding providers out of the box -- from cloud APIs to fully local models:</p> Provider Install OpenAI (default) <code>memsearch</code> (included) Google Gemini <code>memsearch[google]</code> Voyage AI <code>memsearch[voyage]</code> Ollama (local) <code>memsearch[ollama]</code> sentence-transformers (local) <code>memsearch[local]</code> <p>For fully local operation with no API keys, install <code>memsearch[ollama]</code> or <code>memsearch[local]</code>. See Getting Started for API key setup and provider details.</p>"},{"location":"#milvus-backend","title":"Milvus Backend","text":"<p>memsearch supports three deployment modes -- just change the URI:</p> Mode URI Use Case Milvus Lite (default) <code>~/.memsearch/milvus.db</code> Local file, zero config, single user Milvus Server <code>http://localhost:19530</code> Self-hosted, multi-agent, team use Zilliz Cloud <code>https://in03-xxx.zillizcloud.com</code> Fully managed, auto-scaling <p>See Getting Started for connection examples and Docker setup instructions.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>memsearch uses a layered configuration system (lowest \u2192 highest priority):</p> <p>Built-in defaults \u2192 Global config (<code>~/.memsearch/config.toml</code>) \u2192 Project config (<code>.memsearch.toml</code>) \u2192 CLI flags</p> <pre><code>$ memsearch config init               # Interactive wizard\n$ memsearch config set milvus.uri http://localhost:19530\n$ memsearch config list --resolved    # Show merged config from all sources\n</code></pre> <p>API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>) are read from standard environment variables by their respective SDKs -- they are not stored in config files. See Getting Started for the full configuration guide.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This page explains the architecture, design philosophy, and key implementation decisions behind memsearch.</p>"},{"location":"architecture/#design-philosophy","title":"Design Philosophy","text":""},{"location":"architecture/#markdown-as-the-source-of-truth","title":"Markdown as the Source of Truth","text":"<p>The foundational principle of memsearch is simple: markdown files are the canonical data store. The vector database is a derived index -- it can be dropped and rebuilt at any time from the markdown files on disk. This is the same philosophy used by OpenClaw's memory system, and memsearch is designed as a standalone library inspired by that architecture.</p> <p>Why markdown?</p> <ul> <li>Human-readable. Any developer can open a memory file in any text editor and understand what the agent knows. There is no binary format to decode, no special viewer required.</li> <li>Git-friendly. Markdown diffs are clean and meaningful. You get full version history, blame, branching, and merge conflict resolution for free -- the same tools you already use for code.</li> <li>Zero vendor lock-in. Markdown is a plain-text format that has been stable for decades. If you stop using memsearch tomorrow, your knowledge base is still right there on disk, fully intact.</li> <li>Trivially portable. Copy the files to another machine, another tool, another agent framework. No export step, no migration script, no schema translation.</li> </ul> <p>Why NOT a database as the source of truth?</p> <ul> <li>Opaque. Database files are binary blobs that require specific software to read. If the tool disappears, so does easy access to your data.</li> <li>Vendor lock-in. Each database engine has its own storage format, query language, and migration tooling. Switching costs are high.</li> <li>Fragile. Database corruption, version incompatibilities, and backup complexity are real operational concerns for what should be a simple knowledge store.</li> </ul> <p>In memsearch, the vector store is an acceleration layer -- nothing more. If the Milvus database is lost, corrupted, or simply out of date, a single <code>memsearch index</code> command rebuilds the entire index from the markdown files.</p> <pre><code>graph LR\n    MD[\"Markdown Files&lt;br&gt;(source of truth)\"] --&gt;|index| MIL[(Milvus&lt;br&gt;derived index)]\n    MIL --&gt;|lost or corrupted?| REBUILD[\"memsearch index&lt;br&gt;(full rebuild)\"]\n    REBUILD --&gt; MIL\n\n    style MD fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style MIL fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre>"},{"location":"architecture/#inspired-by-openclaw","title":"Inspired by OpenClaw","text":"<p>memsearch follows OpenClaw's memory architecture precisely:</p> Concept OpenClaw memsearch Memory layout <code>MEMORY.md</code> + <code>memory/YYYY-MM-DD.md</code> Same Chunk ID format <code>hash(source:startLine:endLine:contentHash:model)</code> Same Dedup strategy Content-hash primary key Same Compact target Append to daily markdown log Same Source of truth Markdown files (vector DB is derived) Same File watch debounce 1500ms Same default <p>If you are already using OpenClaw's memory directory layout, memsearch works with it directly -- no migration needed.</p>"},{"location":"architecture/#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"architecture/#search-flow","title":"Search Flow","text":"<p>When a query arrives, it is embedded into a vector, then used for hybrid search (dense cosine similarity + BM25 full-text) against the Milvus collection. Results are reranked using Reciprocal Rank Fusion (RRF) and returned with source metadata.</p> <pre><code>graph LR\n    Q[/\"Query\"/] --&gt; E[Embed query] --&gt; HS[\"Hybrid Search&lt;br&gt;(Dense + BM25)\"]\n    HS --&gt; RRF[\"RRF Reranker&lt;br&gt;(k=60)\"] --&gt; R[Top-K Results]\n\n    subgraph Milvus\n        HS\n        RRF\n    end</code></pre>"},{"location":"architecture/#ingest-flow","title":"Ingest Flow","text":"<p>Markdown files are scanned, chunked by headings, and deduplicated using SHA-256 content hashes. Only new or changed chunks are sent to the embedding API and upserted into Milvus. Chunks from deleted files are automatically cleaned up.</p> <pre><code>graph LR\n    F[\"Markdown files\"] --&gt; SC[Scanner] --&gt; C[Chunker] --&gt; D{\"Dedup&lt;br&gt;(SHA-256)\"}\n    D --&gt;|new| E[Embed &amp; Upsert]\n    D --&gt;|exists| S[Skip]\n    D --&gt;|stale| DEL[Delete from Milvus]</code></pre>"},{"location":"architecture/#watch-and-compact","title":"Watch and Compact","text":"<p>The file watcher monitors directories for markdown changes and automatically re-indexes modified files. The compact operation compresses indexed chunks into an LLM-generated summary and writes it back to a daily markdown log -- which the watcher then picks up and indexes, closing the loop.</p> <pre><code>graph LR\n    W[File Watcher] --&gt;|1500ms debounce| I[Auto re-index]\n    FL[Compact] --&gt; L[LLM Summarize] --&gt; MD[\"memory/YYYY-MM-DD.md\"]\n    MD -.-&gt;|triggers| W</code></pre>"},{"location":"architecture/#chunking-strategy","title":"Chunking Strategy","text":"<p>memsearch splits markdown files into semantic chunks using a heading-based strategy, with paragraph-level fallback for oversized sections.</p>"},{"location":"architecture/#heading-based-chunking","title":"Heading-Based Chunking","text":"<p>The chunker treats markdown headings (<code>#</code> through <code>######</code>) as natural chunk boundaries. Each heading and the content below it (up to the next heading of equal or higher level) becomes one chunk. Content before the first heading (the \"preamble\") is treated as its own chunk.</p> <pre><code># Project Notes                    &lt;-- preamble chunk starts here\n\nSome introductory text.\n\n## Redis Configuration              &lt;-- chunk boundary\n\nWe chose Redis for caching...\n\n### Connection Settings              &lt;-- chunk boundary\n\nhost=localhost, port=6379...\n\n## Authentication                    &lt;-- chunk boundary\n\nWe use JWT tokens...\n</code></pre>"},{"location":"architecture/#paragraph-based-splitting-for-large-sections","title":"Paragraph-Based Splitting for Large Sections","text":"<p>When a heading-delimited section exceeds <code>max_chunk_size</code> (default: 1500 characters), the chunker splits it further at paragraph boundaries (blank lines). A configurable <code>overlap_lines</code> (default: 2 lines) is carried forward between sub-chunks to preserve context continuity.</p>"},{"location":"architecture/#chunk-metadata","title":"Chunk Metadata","text":"<p>Each chunk carries rich metadata for provenance tracking:</p> Field Description <code>content</code> The raw text of the chunk <code>source</code> Absolute file path the chunk was extracted from <code>heading</code> The nearest heading text (empty string for preamble) <code>heading_level</code> Heading depth: 1--6 for <code>#</code>--<code>######</code>, 0 for preamble <code>start_line</code> First line number in the source file (1-indexed) <code>end_line</code> Last line number in the source file <code>content_hash</code> Truncated SHA-256 hash of the chunk content (16 hex chars)"},{"location":"architecture/#deduplication","title":"Deduplication","text":"<p>memsearch uses content-addressable storage to avoid redundant embedding API calls and duplicate data in the vector store.</p>"},{"location":"architecture/#how-it-works","title":"How It Works","text":"<ol> <li>Each chunk's content is hashed with SHA-256 (truncated to 16 hex characters).</li> <li>A composite chunk ID is computed from the source path, line range, content hash, and embedding model name -- matching OpenClaw's format: <code>hash(markdown:source:startLine:endLine:contentHash:model)</code>.</li> <li>Before embedding, the set of existing chunk IDs for the source file is queried from Milvus.</li> <li>Only chunks whose composite ID is not already present get embedded and upserted.</li> <li>Chunks whose composite ID no longer appears in the re-chunked file are deleted (stale chunk cleanup).</li> </ol> <pre><code>graph TD\n    C[\"Chunk content\"] --&gt; H[\"SHA-256&lt;br&gt;(content_hash)\"]\n    H --&gt; CID[\"Composite ID&lt;br&gt;hash(source:lines:contentHash:model)\"]\n    CID --&gt; CHECK{\"Exists in&lt;br&gt;Milvus?\"}\n    CHECK --&gt;|No| EMBED[\"Embed &amp; Upsert\"]\n    CHECK --&gt;|Yes| SKIP[\"Skip&lt;br&gt;(save API cost)\"]</code></pre>"},{"location":"architecture/#why-this-matters","title":"Why This Matters","text":"<ul> <li>No external cache needed. The hash IS the primary key in Milvus. There is no SQLite sidecar database, no Redis cache, no <code>.json</code> tracking file. The deduplication mechanism is the storage key itself.</li> <li>Incremental indexing. Re-running <code>memsearch index</code> on an unchanged knowledge base produces zero embedding API calls. Only genuinely new or modified content is processed.</li> <li>Cost savings. Embedding API calls are the primary cost of running a semantic search system. Content-addressable dedup ensures you never pay to embed the same content twice.</li> </ul>"},{"location":"architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"architecture/#collection-schema","title":"Collection Schema","text":"<p>All chunks are stored in a single Milvus collection named <code>memsearch_chunks</code> (configurable). The schema uses both dense and sparse vector fields to enable hybrid search:</p> Field Type Purpose <code>chunk_hash</code> <code>VARCHAR(64)</code> Primary key -- composite SHA-256 chunk ID <code>embedding</code> <code>FLOAT_VECTOR</code> Dense embedding from the configured provider <code>content</code> <code>VARCHAR(65535)</code> Raw chunk text (also feeds BM25 via Milvus Function) <code>sparse_vector</code> <code>SPARSE_FLOAT_VECTOR</code> Auto-generated BM25 sparse vector <code>source</code> <code>VARCHAR(1024)</code> File path the chunk was extracted from <code>heading</code> <code>VARCHAR(1024)</code> Nearest heading text <code>heading_level</code> <code>INT64</code> Heading depth (0 = preamble) <code>start_line</code> <code>INT64</code> First line number in source file <code>end_line</code> <code>INT64</code> Last line number in source file <p>The <code>sparse_vector</code> field is populated automatically by a Milvus BM25 Function that processes the <code>content</code> field -- no application-side sparse encoding is needed.</p>"},{"location":"architecture/#hybrid-search","title":"Hybrid Search","text":"<p>Search combines two retrieval strategies and merges their results:</p> <ol> <li>Dense vector search -- cosine similarity on the <code>embedding</code> field (semantic meaning).</li> <li>BM25 sparse search -- keyword matching on the <code>sparse_vector</code> field (exact term overlap).</li> <li>RRF reranking -- Reciprocal Rank Fusion with k=60 merges the two ranked lists into a single result set.</li> </ol> <p>This hybrid approach catches results that pure semantic search might miss (exact names, error codes, configuration values) while still benefiting from the semantic understanding that dense embeddings provide.</p>"},{"location":"architecture/#three-tier-deployment","title":"Three-Tier Deployment","text":"<p>memsearch supports three Milvus deployment modes. Switch between them by changing a single parameter (<code>milvus_uri</code>):</p> <pre><code>graph TD\n    A[\"memsearch\"] --&gt; B{\"milvus_uri\"}\n    B --&gt;|\"~/.memsearch/milvus.db&lt;br&gt;(default)\"| C[\"Milvus Lite&lt;br&gt;Local .db file&lt;br&gt;Zero config\"]\n    B --&gt;|\"http://host:19530\"| D[\"Milvus Server&lt;br&gt;Self-hosted&lt;br&gt;Docker / K8s\"]\n    B --&gt;|\"https://...zillizcloud.com\"| E[\"Zilliz Cloud&lt;br&gt;Fully managed&lt;br&gt;Auto-scaling\"]\n\n    style C fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style D fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style E fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1</code></pre> Tier URI Pattern Use Case Milvus Lite <code>~/.memsearch/milvus.db</code> Personal use, single agent, development. No server to install. Milvus Server <code>http://localhost:19530</code> Multi-agent teams, shared infrastructure, CI/CD. Deploy via Docker or Kubernetes. Zilliz Cloud <code>https://...zillizcloud.com</code> Production SaaS, zero-ops, auto-scaling. Free tier available at cloud.zilliz.com."},{"location":"architecture/#physical-isolation","title":"Physical Isolation","text":"<p>All agents and projects share the same collection name (<code>memsearch_chunks</code>) by default. Physical isolation between agents is achieved by pointing each one to a different <code>milvus_uri</code> -- each agent gets its own Milvus Lite database file, its own Milvus server, or its own Zilliz Cloud cluster. This avoids the complexity of multi-tenant collection management while keeping the schema simple.</p>"},{"location":"architecture/#configuration-system","title":"Configuration System","text":"<p>memsearch uses a 4-layer configuration system. Each layer overrides the one before it:</p> <pre><code>graph LR\n    D[\"1. Defaults\"] --&gt; G[\"2. Global Config&lt;br&gt;~/.memsearch/config.toml\"]\n    G --&gt; P[\"3. Project Config&lt;br&gt;.memsearch.toml\"]\n    P --&gt; C[\"4. CLI Flags&lt;br&gt;--milvus-uri, etc.\"]</code></pre> Priority Source Scope Example 1 (lowest) Built-in defaults Hardcoded <code>milvus.uri = ~/.memsearch/milvus.db</code> 2 <code>~/.memsearch/config.toml</code> User-global Shared across all projects 3 <code>.memsearch.toml</code> Per-project Committed to the repo or gitignored 4 (highest) CLI flags Per-command <code>--milvus-uri http://...</code> <p>Note: API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>) are read from environment variables by their respective SDKs. They are not part of the memsearch configuration system and are never written to config files.</p>"},{"location":"architecture/#config-sections","title":"Config Sections","text":"<p>The full configuration is organized into five sections:</p> <pre><code>[milvus]\nuri = \"~/.memsearch/milvus.db\"\ntoken = \"\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"                           # empty = provider default\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"                       # empty = provider default\nprompt_file = \"\"                     # custom prompt template path\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n</code></pre>"},{"location":"architecture/#data-flow-overview","title":"Data Flow Overview","text":"<p>The following diagram shows the complete data flow from source-of-truth markdown files through processing and into the derived vector store:</p> <pre><code>graph TB\n    subgraph \"Source of Truth\"\n        MEM[\"MEMORY.md\"]\n        D1[\"memory/2026-02-08.md\"]\n        D2[\"memory/2026-02-09.md\"]\n    end\n\n    subgraph \"Processing\"\n        SCAN[Scanner] --&gt; CHUNK[Chunker]\n        CHUNK --&gt; HASH[\"SHA-256&lt;br&gt;Dedup\"]\n    end\n\n    subgraph \"Storage (derived)\"\n        EMB[Embedding API] --&gt; MIL[(Milvus)]\n    end\n\n    MEM &amp; D1 &amp; D2 --&gt; SCAN\n    HASH --&gt;|new chunks| EMB\n    MIL --&gt;|search| RES[Results]\n\n    style MEM fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style D1 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style D2 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style MIL fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre>"},{"location":"architecture/#the-compact-cycle","title":"The Compact Cycle","text":"<p>The compact operation creates a feedback loop that keeps the knowledge base compact:</p> <pre><code>graph LR\n    CHUNKS[\"Indexed chunks&lt;br&gt;in Milvus\"] --&gt; RETRIEVE[\"Retrieve all&lt;br&gt;(or filtered)\"]\n    RETRIEVE --&gt; LLM[\"LLM Summarize&lt;br&gt;(OpenAI / Anthropic / Gemini)\"]\n    LLM --&gt; WRITE[\"Append to&lt;br&gt;memory/YYYY-MM-DD.md\"]\n    WRITE --&gt; WATCH[\"File watcher&lt;br&gt;detects change\"]\n    WATCH --&gt; REINDEX[\"Auto re-index&lt;br&gt;updated file\"]\n    REINDEX --&gt; CHUNKS\n\n    style WRITE fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style CHUNKS fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre> <ol> <li>All (or filtered) chunks are retrieved from Milvus.</li> <li>An LLM compresses them into a concise summary preserving key facts, decisions, and code patterns.</li> <li>The summary is appended to a daily markdown log (<code>memory/YYYY-MM-DD.md</code>).</li> <li>The file watcher detects the change and re-indexes the updated file.</li> <li>The cycle completes: the compressed knowledge is now searchable, and the source-of-truth markdown has the full history.</li> </ol>"},{"location":"architecture/#security","title":"Security","text":""},{"location":"architecture/#local-first-by-default","title":"Local-First by Default","text":"<p>The entire memsearch pipeline runs locally by default:</p> <ul> <li>Milvus Lite stores data in a local <code>.db</code> file on your filesystem.</li> <li>Local embedding providers (<code>memsearch[local]</code> with sentence-transformers, or <code>memsearch[ollama]</code> with a local Ollama server) process text without any network calls.</li> </ul> <p>In a fully local configuration, your data never leaves your machine.</p>"},{"location":"architecture/#when-data-leaves-your-machine","title":"When Data Leaves Your Machine","text":"<p>Data is transmitted externally only when you explicitly choose a remote component:</p> Component Local Option Remote Option Vector store Milvus Lite (default) Milvus Server, Zilliz Cloud Embeddings <code>local</code>, <code>ollama</code> <code>openai</code>, <code>google</code>, <code>voyage</code> Compact LLM Ollama (local) OpenAI, Anthropic, Gemini"},{"location":"architecture/#api-key-handling","title":"API Key Handling","text":"<p>API keys are read from standard environment variables (<code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>VOYAGE_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>). They are never written to config files by memsearch, never logged, and never stored in the vector database.</p>"},{"location":"architecture/#filesystem-access","title":"Filesystem Access","text":"<p>memsearch reads only the directories and files you explicitly configure via <code>paths</code>. It does not scan outside those paths. Hidden files and directories (those starting with <code>.</code>) are skipped by default during scanning.</p>"},{"location":"claude-plugin/","title":"Claude Code Plugin","text":"<p>Automatic persistent memory for Claude Code. No commands to learn, no manual saving -- just install the plugin and Claude remembers what you worked on across sessions.</p> <p>The plugin is built entirely on Claude Code's own primitives: Hooks for lifecycle events, Skills for intelligent retrieval, and CLI for tool access. No MCP servers, no sidecar services, no extra network round-trips. Everything runs locally as shell scripts, a skill definition, and a Python CLI.</p>"},{"location":"claude-plugin/#how-the-pieces-fit-together","title":"How the pieces fit together","text":"<pre><code>graph LR\n    subgraph \"memsearch (Python library)\"\n        LIB[Core: chunker, embeddings,&lt;br/&gt;vector store, scanner]\n    end\n\n    subgraph \"memsearch CLI\"\n        CLI[\"CLI commands:&lt;br/&gt;search \u00b7 index \u00b7 watch&lt;br/&gt;expand \u00b7 transcript \u00b7 config\"]\n    end\n\n    subgraph \"ccplugin (Claude Code Plugin)\"\n        HOOKS[\"Shell hooks:&lt;br/&gt;SessionStart \u00b7 UserPromptSubmit&lt;br/&gt;Stop \u00b7 SessionEnd\"]\n        SKILL[\"Skill:&lt;br/&gt;memory-recall (context: fork)\"]\n    end\n\n    LIB --&gt; CLI\n    CLI --&gt; HOOKS\n    CLI --&gt; SKILL\n    HOOKS --&gt;|\"runs inside\"| CC[Claude Code]\n    SKILL --&gt;|\"subagent\"| CC\n\n    style LIB fill:#1a2744,stroke:#6ba3d6,color:#a8b2c1\n    style CLI fill:#1a2744,stroke:#e0976b,color:#a8b2c1\n    style HOOKS fill:#1a2744,stroke:#7bc67e,color:#a8b2c1\n    style CC fill:#2a1a44,stroke:#c97bdb,color:#a8b2c1</code></pre> <p>The memsearch Python library provides the core engine (chunking, embedding, vector storage, search). The memsearch CLI wraps the library into shell-friendly commands. The Claude Code Plugin ties those CLI commands to Claude Code's hook lifecycle and skill system \u2014 hooks handle session management and memory capture, while the memory-recall skill handles intelligent retrieval in a forked subagent context.</p>"},{"location":"claude-plugin/#without-vs-with-the-plugin","title":"Without vs. With the Plugin","text":"<pre><code>sequenceDiagram\n    participant You\n    participant Claude as Claude Code\n\n    rect rgb(60, 30, 30)\n    note right of You: Without plugin\n    You-&gt;&gt;Claude: Monday: \"Add Redis caching with 5min TTL\"\n    Claude-&gt;&gt;You: \u2705 Done \u2014 implements caching\n    note over Claude: Session ends. Context is gone.\n    You-&gt;&gt;Claude: Wednesday: \"The /orders endpoint is slow\"\n    Claude-&gt;&gt;You: \u274c Suggests solutions from scratch&lt;br/&gt;(forgot about the Redis cache from Monday)\n    end\n\n    rect rgb(20, 50, 30)\n    note right of You: With plugin\n    You-&gt;&gt;Claude: Monday: \"Add Redis caching with 5min TTL\"\n    Claude-&gt;&gt;You: \u2705 Done \u2014 implements caching\n    note over Claude: Plugin auto-summarizes \u2192 memory/2026-02-10.md\n    You-&gt;&gt;Claude: Wednesday: \"The /orders endpoint is slow\"\n    note over Claude: Plugin injects: \"Added Redis caching&lt;br/&gt;middleware with 5min TTL...\"\n    Claude-&gt;&gt;You: \u2705 \"We already have Redis caching \u2014&lt;br/&gt;let me add the /orders endpoint to it\"\n    end</code></pre>"},{"location":"claude-plugin/#when-is-this-useful","title":"When Is This Useful?","text":"<ul> <li>Picking up where you left off. You debugged an auth issue yesterday but didn't finish. Today Claude remembers the root cause, which files you touched, and what you tried \u2014 no re-explaining needed.</li> <li>Recalling past decisions. \"Why did we switch from JWT to session cookies?\" Claude can trace back to the original conversation where the trade-offs were discussed, thanks to the 3-layer progressive disclosure that drills from summary \u2192 full section \u2192 original transcript.</li> <li>Long-running projects. Over days or weeks of development, architectural context accumulates automatically. Claude stays aware of your codebase conventions, past refactors, and resolved issues without you having to maintain a manual changelog.</li> </ul>"},{"location":"claude-plugin/#quick-start","title":"Quick Start","text":""},{"location":"claude-plugin/#install-from-marketplace-recommended","title":"Install from Marketplace (recommended)","text":"<pre><code># 1. Set your embedding API key (OpenAI is the default provider)\nexport OPENAI_API_KEY=\"sk-...\"\n\n# 2. In Claude Code, add the marketplace and install the plugin\n/plugin marketplace add zilliztech/memsearch\n/plugin install memsearch\n\n# 3. Have a conversation, then exit. Check your memories:\ncat .memsearch/memory/$(date +%Y-%m-%d).md\n\n# 4. Start a new session -- Claude automatically remembers!\n</code></pre> <p>Note: If memsearch is not already installed, the plugin will attempt to install it automatically on first run.</p>"},{"location":"claude-plugin/#how-it-works","title":"How It Works","text":"<p>The plugin hooks into 4 Claude Code lifecycle events and provides a memory-recall skill. A singleton <code>memsearch watch</code> process runs in the background, keeping the vector index in sync with markdown files as they change. (Milvus Lite falls back to one-time indexing at session start.)</p>"},{"location":"claude-plugin/#lifecycle-diagram","title":"Lifecycle Diagram","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; SessionStart\n    SessionStart --&gt; WatchRunning: start memsearch watch\n    SessionStart --&gt; InjectRecent: load recent memories (cold start)\n\n    state WatchRunning {\n        [*] --&gt; Watching\n        Watching --&gt; Reindex: file changed\n        Reindex --&gt; Watching: done\n    }\n\n    InjectRecent --&gt; Prompting\n\n    state Prompting {\n        [*] --&gt; UserInput\n        UserInput --&gt; Hint: UserPromptSubmit hook\n        Hint --&gt; ClaudeProcesses: \"[memsearch] Memory available\"\n        ClaudeProcesses --&gt; MemoryRecall: needs context?\n        MemoryRecall --&gt; Subagent: memory-recall skill [fork]\n        Subagent --&gt; ClaudeResponds: curated summary\n        ClaudeProcesses --&gt; ClaudeResponds: no memory needed\n        ClaudeResponds --&gt; UserInput: next turn\n        ClaudeResponds --&gt; Summary: Stop hook (async, non-blocking)\n        Summary --&gt; WriteMD: append to YYYY-MM-DD.md\n    }\n\n    Prompting --&gt; SessionEnd: user exits\n    SessionEnd --&gt; StopWatch: stop memsearch watch\n    StopWatch --&gt; [*]</code></pre>"},{"location":"claude-plugin/#hook-summary","title":"Hook Summary","text":"<p>The plugin defines exactly 4 hooks, all declared in <code>hooks/hooks.json</code>:</p> Hook Type Async Timeout What It Does SessionStart command no 10s Start <code>memsearch watch</code> singleton, write session heading to today's <code>.md</code>, inject recent daily logs as cold-start context via <code>additionalContext</code>, display config status (provider/model/milvus) in <code>systemMessage</code> UserPromptSubmit command no 15s Lightweight hint: returns <code>systemMessage</code> \"[memsearch] Memory available\" (skip if &lt; 10 chars). No search \u2014 recall is handled by the memory-recall skill Stop command yes 120s Extract last turn from transcript with <code>parse-transcript.sh</code>, call <code>claude -p --model haiku</code> to summarize as third-person notes, append summary with session/turn anchors to daily <code>.md</code> SessionEnd command no 10s Stop the <code>memsearch watch</code> background process (cleanup)"},{"location":"claude-plugin/#what-each-hook-does","title":"What Each Hook Does","text":""},{"location":"claude-plugin/#sessionstart","title":"SessionStart","text":"<p>Fires once when a Claude Code session begins. This hook:</p> <ol> <li>Reads config and checks API key. Runs <code>memsearch config get</code> to read the configured embedding provider, model, and Milvus URI. Checks whether the required API key is set for the provider (<code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>VOYAGE_API_KEY</code>; <code>ollama</code> and <code>local</code> need no key). If missing, shows an error in <code>systemMessage</code> and exits early.</li> <li>Starts the watcher. Launches <code>memsearch watch .memsearch/memory/</code> as a singleton background process (PID file lock prevents duplicates). The watcher monitors markdown files and auto-re-indexes on changes with a 1500ms debounce. Milvus Lite falls back to a one-time <code>memsearch index</code> at session start.</li> <li>Writes a session heading. Appends <code>## Session HH:MM</code> to today's memory file (<code>.memsearch/memory/YYYY-MM-DD.md</code>), creating the file if it does not exist.</li> <li>Injects cold-start context. Reads the last 30 lines from the 2 most recent daily logs and returns them as <code>additionalContext</code>. This gives Claude awareness of recent sessions, which helps it decide when to invoke the memory-recall skill.</li> <li>Checks for updates. Queries PyPI (2s timeout) and compares with the installed version. If a newer version is available, appends an <code>UPDATE</code> hint to the status line.</li> <li>Displays config status. Every exit path returns a <code>systemMessage</code> showing the active configuration, e.g. <code>[memsearch v0.1.10] embedding: openai/text-embedding-3-small | milvus: ~/.memsearch/milvus.db | collection: ms_my_app_a1b2c3d4</code> (with <code>| UPDATE: v0.1.12 available</code> when outdated).</li> </ol>"},{"location":"claude-plugin/#userpromptsubmit","title":"UserPromptSubmit","text":"<p>Fires on every user prompt before Claude processes it. This hook:</p> <ol> <li>Extracts the prompt from the hook input JSON.</li> <li>Skips short prompts (under 10 characters) -- greetings and single words don't need memory hints.</li> <li>Returns a lightweight hint. Outputs <code>systemMessage: \"[memsearch] Memory available\"</code> -- a visible one-liner that keeps Claude aware of the memory system without performing any search.</li> </ol> <p>The actual memory retrieval is handled by the memory-recall skill, which Claude invokes automatically when it judges the user's question needs historical context.</p>"},{"location":"claude-plugin/#stop","title":"Stop","text":"<p>Fires after Claude finishes each response. Runs asynchronously so it does not block the user. This hook:</p> <ol> <li>Guards against recursion. Checks <code>stop_hook_active</code> to prevent infinite loops (since the hook itself calls <code>claude -p</code>).</li> <li>Validates the transcript. Skips if the transcript file is missing or has fewer than 3 lines.</li> <li>Parses the last turn. Calls <code>parse-transcript.sh</code>, which:<ul> <li>Scans backward from EOF to find the last real user message (content is a string, not a <code>tool_result</code>)</li> <li>Extracts only the last turn: from that user message to EOF</li> <li>Skips <code>progress</code>, <code>file-history-snapshot</code>, <code>system</code>, and <code>thinking</code> blocks</li> <li>Keeps user/assistant text and tool call summaries; truncates tool results to 1000 characters</li> <li>Uses Python 3 (no <code>jq</code> dependency)</li> </ul> </li> <li>Summarizes with Haiku. Pipes the parsed last turn to <code>claude -p --model haiku --no-session-persistence</code> with a third-person note-taker system prompt that requests 2-6 bullet points recording what the user asked and what Claude did (tools called, files changed, key findings). The summary language matches the user's language.</li> <li>Appends to daily log. Writes a <code>### HH:MM</code> sub-heading with an HTML comment anchor containing session ID, turn UUID, and transcript path. Then explicitly runs <code>memsearch index</code> to ensure the new content is indexed immediately, rather than relying on the watcher's debounce timer (which may not fire before SessionEnd kills the watcher).</li> </ol>"},{"location":"claude-plugin/#sessionend","title":"SessionEnd","text":"<p>Fires when the user exits Claude Code. Simply calls <code>stop_watch</code> to kill the <code>memsearch watch</code> process and clean up the PID file, including a sweep for any orphaned processes.</p>"},{"location":"claude-plugin/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>Memory retrieval uses a three-layer progressive disclosure model, all handled autonomously by the memory-recall skill running in a forked subagent context. Claude invokes the skill when it judges the user's question needs historical context -- no manual intervention required.</p> <pre><code>graph TD\n    SKILL[\"memory-recall skill&lt;br/&gt;(context: fork subagent)\"]\n    SKILL --&gt; L1[\"L1: Search&lt;br/&gt;(memsearch search)\"]\n    L1 --&gt; L2[\"L2: Expand&lt;br/&gt;(memsearch expand)\"]\n    L2 --&gt; L3[\"L3: Transcript drill-down&lt;br/&gt;(memsearch transcript)\"]\n    L3 --&gt; RETURN[\"Curated summary&lt;br/&gt;\u2192 main agent\"]\n\n    style SKILL fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style L1 fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style L2 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style L3 fill:#2a3a5c,stroke:#d66b6b,color:#a8b2c1\n    style RETURN fill:#2a3a5c,stroke:#7bc67e,color:#a8b2c1</code></pre>"},{"location":"claude-plugin/#how-the-skill-works","title":"How the Skill Works","text":"<p>When Claude detects that a user's question could benefit from past context, it automatically invokes the <code>memory-recall</code> skill. The skill runs in a forked subagent context (<code>context: fork</code>), meaning it has its own context window and does not pollute the main conversation. The subagent:</p> <ol> <li>Searches for relevant memories using <code>memsearch search</code></li> <li>Evaluates which results are truly relevant (skips noise)</li> <li>Expands promising results with <code>memsearch expand</code> to get full markdown sections</li> <li>Drills into transcripts when needed with <code>memsearch transcript</code></li> <li>Returns a curated summary to the main agent</li> </ol> <p>The main agent only sees the final summary -- all intermediate search results, raw expand output, and transcript parsing happen inside the subagent.</p> <p>Users can also manually invoke the skill with <code>/memory-recall &lt;query&gt;</code> if Claude doesn't trigger it automatically.</p>"},{"location":"claude-plugin/#l1-search","title":"L1: Search","text":"<p>The subagent runs <code>memsearch search</code> to find relevant chunks from the indexed memory files.</p>"},{"location":"claude-plugin/#l2-expand","title":"L2: Expand","text":"<p>For promising search results, the subagent runs <code>memsearch expand</code> to retrieve the full markdown section surrounding a chunk:</p> <pre><code>$ memsearch expand 7a3f9b21e4c08d56\n</code></pre> <p>Example output:</p> <pre><code>Source: .memsearch/memory/2026-02-10.md (lines 12-32)\nHeading: 09:15\nSession: abc123de-f456-7890-abcd-ef1234567890\nTurn: def456ab-cdef-1234-5678-90abcdef1234\nTranscript: /home/user/.claude/projects/.../abc123de...7890.jsonl\n\n### 08:50\n&lt;!-- session:abc123de... turn:aaa11122... transcript:/.../abc123de...7890.jsonl --&gt;\n- Set up project scaffolding for the new API service\n- Configured FastAPI with uvicorn, added health check endpoint\n- Connected to PostgreSQL via SQLAlchemy async engine\n\n### 09:15\n&lt;!-- session:abc123de... turn:def456ab... transcript:/.../abc123de...7890.jsonl --&gt;\n- Added Redis caching middleware to API with 5-minute TTL\n- Used redis-py async client with connection pooling (max 10 connections)\n- Cache key format: `api:v1:{endpoint}:{hash(params)}`\n- Added cache hit/miss Prometheus counters for monitoring\n- Wrote integration tests with fakeredis\n</code></pre> <p>The subagent sees the full context including neighboring sections. The embedded <code>&lt;!-- session:... --&gt;</code> anchors link to the original conversation -- if the subagent needs to go even deeper, it moves to L3.</p> <p>Additional flags:</p> <pre><code># JSON output with anchor metadata (for programmatic L3 drill-down)\nmemsearch expand 47b5475122b992b6 --json-output\n\n# Show N lines of context before/after instead of the full section\nmemsearch expand 47b5475122b992b6 --lines 10\n</code></pre>"},{"location":"claude-plugin/#l3-transcript-drill-down","title":"L3: Transcript Drill-Down","text":"<p>When Claude needs the original conversation verbatim -- for instance, to recall exact code snippets, error messages, or tool outputs -- it drills into the JSONL transcript.</p> <p>List all turns in a session:</p> <pre><code>$ memsearch transcript /path/to/session.jsonl\n</code></pre> <pre><code>All turns (73):\n\n  6d6210b7-b84  08:50:14  Set up the project scaffolding for...          [12 tools]\n  3075ee94-0f6  09:05:22  Can you add a health check endpoint?\n  8e45ce0d-9a0  09:15:03  Add a Redis caching layer to the API...        [8 tools]\n  53f5cac3-6d9  09:32:41  The cache TTL should be configurable...         [3 tools]\n  c708b40c-8f8  09:45:18  Let's add Prometheus metrics for cache...      [10 tools]\n</code></pre> <p>Each line shows the turn UUID prefix, timestamp, content preview, and how many tool calls occurred.</p> <p>Drill into a specific turn with surrounding context:</p> <pre><code>$ memsearch transcript /path/to/session.jsonl --turn 8e45ce0d --context 1\n</code></pre> <pre><code>Showing 2 turns around 8e45ce0d:\n\n&gt;&gt;&gt; [09:05:22] 3075ee94\nCan you add a health check endpoint?\n\n**Assistant**: Sure, I'll add a `/health` endpoint that checks the database\nconnection and returns the service version.\n\n&gt;&gt;&gt; [09:15:03] 8e45ce0d\nAdd a Redis caching layer to the API with a 5-minute TTL.\n\n**Assistant**: I'll add Redis caching middleware. Let me first check\nyour current dependencies and middleware setup.\n  [Read] requirements.txt\n  [Read] src/middleware/__init__.py\n  [Write] src/middleware/cache.py\n  [Edit] src/main.py \u2014 added cache middleware to app\n</code></pre> <p>This recovers the full original conversation -- user messages, assistant responses, and tool call summaries -- so Claude can recall exactly what happened during a past session.</p> <pre><code># JSON output for programmatic use\nmemsearch transcript /path/to/session.jsonl --turn 6d6210b7 --json-output\n</code></pre>"},{"location":"claude-plugin/#what-the-jsonl-looks-like","title":"What the JSONL Looks Like","text":"<p>The transcript files are standard JSON Lines -- one JSON object per line. Claude Code writes every message, tool call, and tool result as a separate line. Here is what the key message types look like (abbreviated for readability):</p> <p>User message (human input):</p> <pre><code>{\n  \"type\": \"user\",\n  \"uuid\": \"6d6210b7-b841-4cd7-a97f-e3c8bb185d06\",\n  \"parentUuid\": \"8404eaca-3926-4765-bcb9-6ca4befae466\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:14.284Z\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Add a Redis caching layer to the API with a 5-minute TTL.\"\n  }\n}\n</code></pre> <p>Assistant message (text response):</p> <pre><code>{\n  \"type\": \"assistant\",\n  \"uuid\": \"32da9357-1efe-4985-8a6e-4864bbf58951\",\n  \"parentUuid\": \"d99f255c-6ac7-43fa-bcc8-c0dabc4c65cf\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:36.510Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\"type\": \"text\", \"text\": \"I'll add Redis caching middleware. Let me check your current setup.\"}\n    ]\n  }\n}\n</code></pre> <p>Assistant message (tool call):</p> <pre><code>{\n  \"type\": \"assistant\",\n  \"uuid\": \"35fa9333-02ff-4b07-9036-ec0e3e290602\",\n  \"parentUuid\": \"7ab167db-9a57-4f51-b5d3-eb63a2e6a5ad\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:20.992Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_014CPfherKZMyYbbG5VT4dyX\",\n        \"name\": \"Read\",\n        \"input\": {\"file_path\": \"/path/to/src/middleware/__init__.py\"}\n      }\n    ]\n  }\n}\n</code></pre> <p>Tool result (returned to assistant as a user message):</p> <pre><code>{\n  \"type\": \"user\",\n  \"uuid\": \"7dd5ac66-c848-4e39-952a-511c94ac66f2\",\n  \"parentUuid\": \"35fa9333-02ff-4b07-9036-ec0e3e290602\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:21.005Z\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_014CPfherKZMyYbbG5VT4dyX\",\n        \"content\": \"     1\u2192from .logging import LoggingMiddleware\\n     2\u2192\\n     3\u2192...\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Key fields:</p> Field Description <code>type</code> Message type: <code>user</code>, <code>assistant</code>, <code>progress</code>, <code>system</code>, <code>file-history-snapshot</code> <code>uuid</code> Unique ID for this message <code>parentUuid</code> ID of the previous message (forms a linked chain) <code>sessionId</code> Session ID (matches the JSONL filename) <code>timestamp</code> ISO 8601 timestamp <code>message.content</code> String for user text, or array of <code>text</code> / <code>tool_use</code> / <code>tool_result</code> blocks <p>You don't need to parse JSONL manually</p> <p>The <code>memsearch transcript</code> command handles all the parsing, truncation, and formatting. The JSONL structure is documented here for transparency -- most users will never need to read these files directly.</p>"},{"location":"claude-plugin/#session-anchors","title":"Session Anchors","text":"<p>Each memory summary includes an HTML comment anchor that links the chunk back to its source session, enabling the L2-to-L3 drill-down:</p> <pre><code>### 14:30\n&lt;!-- session:abc123def turn:ghi789jkl transcript:/home/user/.claude/projects/.../abc123def.jsonl --&gt;\n- Implemented caching system with Redis L1 and in-process LRU L2\n- Fixed N+1 query issue in order-service using selectinload\n- Decided to use Prometheus counters for cache hit/miss metrics\n</code></pre> <p>The anchor contains three fields:</p> Field Description <code>session</code> Claude Code session ID (also the JSONL filename without extension) <code>turn</code> UUID of the last user turn in the session <code>transcript</code> Absolute path to the JSONL transcript file <p>Claude extracts these fields from <code>memsearch expand --json-output</code> and uses them to call <code>memsearch transcript</code> for L3 access.</p>"},{"location":"claude-plugin/#memory-storage","title":"Memory Storage","text":"<p>All memories live in <code>.memsearch/memory/</code> inside your project directory.</p>"},{"location":"claude-plugin/#directory-structure","title":"Directory Structure","text":"<pre><code>your-project/\n\u251c\u2500\u2500 .memsearch/\n\u2502   \u251c\u2500\u2500 .watch.pid            &lt;-- singleton watcher PID file\n\u2502   \u2514\u2500\u2500 memory/\n\u2502       \u251c\u2500\u2500 2026-02-07.md     &lt;-- daily memory log\n\u2502       \u251c\u2500\u2500 2026-02-08.md\n\u2502       \u2514\u2500\u2500 2026-02-09.md     &lt;-- today's session summaries\n\u2514\u2500\u2500 ... (your project files)\n</code></pre>"},{"location":"claude-plugin/#example-memory-file","title":"Example Memory File","text":"<p>A typical daily memory file (<code>2026-02-09.md</code>) looks like this:</p> <pre><code>## Session 14:30\n\n### 14:30\n&lt;!-- session:abc123def turn:ghi789jkl transcript:/home/user/.claude/projects/.../abc123def.jsonl --&gt;\n- Implemented caching system with Redis L1 and in-process LRU L2\n- Fixed N+1 query issue in order-service using selectinload\n- Decided to use Prometheus counters for cache hit/miss metrics\n\n## Session 17:45\n\n### 17:45\n&lt;!-- session:mno456pqr turn:stu012vwx transcript:/home/user/.claude/projects/.../mno456pqr.jsonl --&gt;\n- Debugged React hydration mismatch caused by Date.now() during SSR\n- Added comprehensive test suite for the caching middleware\n- Reviewed PR #42: approved with minor naming suggestions\n</code></pre> <p>Each file accumulates all sessions from that day. The format is plain markdown -- human-readable, <code>grep</code>-able, and git-friendly.</p>"},{"location":"claude-plugin/#markdown-is-the-source-of-truth","title":"Markdown Is the Source of Truth","text":"<p>The Milvus vector index is a derived cache that can be rebuilt at any time:</p> <pre><code>memsearch index .memsearch/memory/\n</code></pre> <p>This means:</p> <ul> <li>No data loss. Even if Milvus is corrupted or deleted, your memories are safe in <code>.md</code> files.</li> <li>Portable. Copy <code>.memsearch/memory/</code> to another machine and rebuild the index.</li> <li>Auditable. You can read, edit, or delete any memory entry with a text editor.</li> <li>Git-friendly. Commit your memory files to version control for a complete project history.</li> </ul>"},{"location":"claude-plugin/#comparison-with-claude-mem","title":"Comparison with claude-mem","text":"<p>claude-mem is another memory solution for Claude Code. Here is a detailed comparison:</p> Aspect memsearch claude-mem Architecture 4 shell hooks + 1 skill + 1 watch process 5 JS hooks + 1 skill + MCP tools + Express worker service (port 37777) + React viewer Integration Native hooks + skill + CLI -- no MCP, no sidecar service Hooks + skill + MCP tools + HTTP worker service Memory recall Skill in forked subagent -- <code>memory-recall</code> runs in <code>context: fork</code>, intermediate results stay isolated from main context Skill + MCP hybrid -- <code>mem-search</code> skill for auto-recall, plus 5 MCP tools (<code>search</code>, <code>timeline</code>, <code>get_observations</code>, <code>save_memory</code>, ...) for explicit access Progressive disclosure 3-layer in subagent: search \u2192 expand \u2192 transcript, all in forked context -- only curated summary reaches main conversation 3-layer: <code>mem-search</code> skill for auto-recall; MCP tools for explicit drill-down Session capture 1 async <code>claude -p --model haiku</code> call at session end AI observation compression on every tool use (<code>PostToolUse</code> hook) + session summary Vector backend Milvus -- hybrid search (dense + BM25 + RRF), scales from embedded to distributed cluster ChromaDB -- dense only; SQLite FTS5 for keyword search (separate, not fused) Embedding model Pluggable: OpenAI, Google, Voyage, Ollama, local Fixed: all-MiniLM-L6-v2 (384-dim, WASM backend) Storage format Transparent <code>.md</code> files -- human-readable, git-friendly SQLite database + ChromaDB binary Data portability Copy <code>.memsearch/memory/*.md</code> and rebuild index Export from SQLite + ChromaDB Runtime dependency Python (<code>memsearch</code> CLI) + <code>claude</code> CLI Node.js / Bun + Express worker service Context window cost No MCP tool definitions; skill runs in forked context -- only curated summary enters main context MCP tool definitions permanently loaded + each MCP tool call/result consumes main context"},{"location":"claude-plugin/#the-key-difference-forked-subagent-vs-mcp-tools","title":"The Key Difference: Forked Subagent vs. MCP Tools","text":"<p>Both projects use hooks for session lifecycle and skills for memory recall. The architectural divergence is in how retrieval interacts with the main context window.</p> <p>memsearch runs memory recall in a forked subagent (<code>context: fork</code>). The <code>memory-recall</code> skill gets its own isolated context window -- all search, expand, and transcript operations happen there. Only the curated summary is returned to the main conversation. This means: (1) intermediate search results never pollute the main context, (2) multi-step retrieval is autonomous, and (3) no MCP tool definitions consume context tokens.</p> <p>claude-mem combines a <code>mem-search</code> skill with MCP tools (<code>search</code>, <code>timeline</code>, <code>get_observations</code>, <code>save_memory</code>). The MCP tools give Claude explicit control over memory access in the main conversation, at the cost of tool definitions permanently consuming context tokens. The <code>PostToolUse</code> hook also records every tool call as an observation, providing richer per-action granularity but incurring more API calls.</p> <p>The other key difference is storage philosophy: memsearch treats markdown files as the source of truth (human-readable, git-friendly, rebuildable), while claude-mem uses SQLite + ChromaDB (opaque but structured, with richer queryable metadata).</p>"},{"location":"claude-plugin/#comparison-with-claudes-native-memory","title":"Comparison with Claude's Native Memory","text":"<p>Claude Code has built-in memory features: <code>CLAUDE.md</code> files and auto-memory (the <code>/memory</code> command). Here is why memsearch provides a stronger solution:</p> Aspect Claude Native Memory memsearch Storage Single <code>CLAUDE.md</code> file (or per-project) Unlimited daily <code>.md</code> files with full history Recall mechanism File is loaded at session start (no search) Skill-based semantic search -- Claude auto-invokes when context is needed Granularity One monolithic file, manually edited Per-session bullet points, automatically generated Search None -- Claude reads the whole file or nothing Hybrid semantic search (dense + BM25) returning top-k relevant chunks History depth Limited to what fits in one file Unlimited -- every session is logged, every entry is searchable Automatic capture <code>/memory</code> command requires manual intervention Fully automatic -- hooks capture every session Progressive disclosure None -- entire file is loaded into context 3-layer model (L1 auto-inject, L2 expand, L3 transcript) minimizes context usage Deduplication Manual -- user must avoid adding duplicates SHA-256 content hashing prevents duplicate embeddings Portability Tied to Claude Code's internal format Standard markdown files, usable with any tool"},{"location":"claude-plugin/#why-this-matters","title":"Why This Matters","text":"<p><code>CLAUDE.md</code> is a blunt instrument: it loads the entire file into context at session start, regardless of relevance. As the file grows, it wastes context window on irrelevant information and eventually hits size limits. There is no search -- Claude cannot selectively recall a specific decision from three weeks ago.</p> <p>memsearch solves this with skill-based semantic search and progressive disclosure. When Claude judges that historical context would help, it auto-invokes the memory-recall skill, which runs in a forked subagent and autonomously searches, expands, and curates relevant memories. History can grow indefinitely without degrading performance, because the vector index handles the filtering. And the three-layer model (search \u2192 expand \u2192 transcript) runs entirely in the subagent, keeping the main context window clean.</p>"},{"location":"claude-plugin/#plugin-files","title":"Plugin Files","text":"<p>The plugin lives in the <code>ccplugin/</code> directory at the root of the memsearch repository:</p> <pre><code>ccplugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json              # Plugin manifest (name, version, description)\n\u251c\u2500\u2500 hooks/\n\u2502   \u251c\u2500\u2500 hooks.json               # Hook definitions (4 lifecycle hooks)\n\u2502   \u251c\u2500\u2500 common.sh                # Shared setup: env, PATH, memsearch detection, watch management\n\u2502   \u251c\u2500\u2500 session-start.sh         # Start watch + write session heading + inject cold-start context\n\u2502   \u251c\u2500\u2500 user-prompt-submit.sh    # Lightweight systemMessage hint (\"[memsearch] Memory available\")\n\u2502   \u251c\u2500\u2500 stop.sh                  # Parse transcript -&gt; haiku summary -&gt; append to daily .md\n\u2502   \u251c\u2500\u2500 parse-transcript.sh      # Deterministic JSONL-to-text parser with truncation\n\u2502   \u2514\u2500\u2500 session-end.sh           # Stop watch process (cleanup)\n\u2514\u2500\u2500 skills/\n    \u2514\u2500\u2500 memory-recall/\n        \u2514\u2500\u2500 SKILL.md             # Memory retrieval skill (context: fork subagent)\n</code></pre>"},{"location":"claude-plugin/#file-descriptions","title":"File Descriptions","text":"File Purpose <code>plugin.json</code> Claude Code plugin manifest. Declares the plugin name (<code>memsearch</code>), version, and description. <code>hooks.json</code> Defines the 4 lifecycle hooks (SessionStart, UserPromptSubmit, Stop, SessionEnd) with their types, timeouts, and async flags. <code>common.sh</code> Shared shell library sourced by all hooks. Handles stdin JSON parsing, PATH setup, memsearch binary detection (prefers PATH, falls back to <code>uv run</code>), memory directory management, and the watch singleton (start/stop with PID file and orphan cleanup). <code>session-start.sh</code> SessionStart hook implementation. Starts the watcher, writes the session heading, and reads recent memory files for cold-start context injection. <code>user-prompt-submit.sh</code> UserPromptSubmit hook implementation. Returns a lightweight <code>systemMessage</code> hint to keep Claude aware of the memory system. No search -- retrieval is handled by the memory-recall skill. <code>stop.sh</code> Stop hook implementation. Extracts the transcript path, validates it, delegates parsing to <code>parse-transcript.sh</code>, calls Haiku for summarization (with <code>CLAUDECODE=</code> to bypass nested session detection), and appends the result with session anchors to the daily memory file. <code>parse-transcript.sh</code> Standalone transcript parser. Extracts the last turn (last user question + all responses to EOF) from a JSONL transcript using Python 3. Skips progress, thinking, and file-history-snapshot entries. No <code>jq</code> dependency. Used by <code>stop.sh</code>. <code>session-end.sh</code> SessionEnd hook implementation. Calls <code>stop_watch</code> to terminate the background watcher and clean up."},{"location":"claude-plugin/#the-memsearch-cli","title":"The <code>memsearch</code> CLI","text":"<p>The plugin is built entirely on the <code>memsearch</code> CLI -- every hook is a shell script calling <code>memsearch</code> subcommands. Here are the commands most relevant to the plugin:</p> Command Used By What It Does <code>search &lt;query&gt;</code> memory-recall skill Semantic search over indexed memories (<code>--top-k</code> for result count, <code>--json-output</code> for JSON) <code>watch &lt;paths&gt;</code> SessionStart hook Background watcher that auto-indexes on file changes (1500ms debounce) <code>index &lt;paths&gt;</code> Manual / rebuild One-shot index of markdown files (<code>--force</code> to re-index all) <code>expand &lt;chunk_hash&gt;</code> memory-recall skill (L2) Show full markdown section around a chunk, with anchor metadata <code>transcript &lt;jsonl&gt;</code> memory-recall skill (L3) Parse Claude Code JSONL transcript into readable conversation turns <code>config init</code> Quick Start Interactive config wizard for first-time setup <code>stats</code> Manual Show index statistics (collection size, chunk count) <code>reset</code> Manual Drop all indexed data (requires <code>--yes</code> to confirm) <p>For the full CLI reference, see the CLI Reference page.</p>"},{"location":"claude-plugin/#development-mode","title":"Development Mode","text":"<p>For contributors or if you want to modify the plugin locally:</p> <pre><code>git clone https://github.com/zilliztech/memsearch.git\ncd memsearch &amp;&amp; uv sync\nclaude --plugin-dir ./ccplugin\n</code></pre>"},{"location":"claude-plugin/#troubleshooting","title":"Troubleshooting","text":"<p>The plugin provides several observability mechanisms, from always-on status lines to opt-in debug logging. Work from the top down -- most issues are resolved by the first two sections.</p> Mechanism Always On? What You See Best For SessionStart status line Yes <code>[memsearch v0.1.11] embedding: openai/... | milvus: ...</code> Config errors, version checks Debug mode No Full hook JSON in <code>~/.claude/logs/</code> Hook execution, additionalContext CLI diagnostic commands Manual Config, index stats, search results Config verification, search testing Watch process Yes (background) PID file at <code>.memsearch/.watch.pid</code> Index sync issues Skill execution Yes (in UI) Skill invocation + Bash tool calls Memory recall debugging Memory files Yes <code>.memsearch/memory/YYYY-MM-DD.md</code> Stop hook, summary quality"},{"location":"claude-plugin/#1-sessionstart-status-line","title":"1. SessionStart Status Line","text":"<p>Every session starts with a status line in <code>systemMessage</code>. This is the first thing to check when something seems wrong.</p> <p>Hooks communicate with Claude Code by returning JSON. Two key fields:</p> <ul> <li><code>systemMessage</code> \u2014 A visible info line shown in the terminal, like a status bar.</li> <li><code>additionalContext</code> \u2014 Invisible to the user; injected into Claude's context silently. Only appears in debug logs (<code>claude --debug</code>).</li> </ul> <p>Here is what a session looks like with the plugin installed:</p> <pre><code>   \u273b\n   |\n  \u259f\u2588\u2599     Claude Code v2.x.x\n\u2590\u259b\u2588\u2588\u2588\u259c\u258c   Model \u00b7 Plan\n\u259d\u259c\u2588\u2588\u2588\u2588\u2588\u259b\u2598  ~/my-project\n \u2598\u2598 \u259d\u259d\n \u23bf  SessionStart:startup says: [memsearch v0.1.11]        \u2190 systemMessage\n    embedding: openai/text-embedding-3-small | milvus:       (SessionStart hook)\n    ~/.memsearch/milvus.db\n\n\u276f How does the caching layer work?\n\n \u23bf  UserPromptSubmit says: [memsearch] Memory available    \u2190 systemMessage\n                                                             (UserPromptSubmit hook)\n\u2736 Thinking\u2026\n</code></pre> <p>The SessionStart hook also loads the 2 most recent daily logs as <code>additionalContext</code> \u2014 Claude reads this silently to decide when to invoke the memory-recall skill, but you won't see it in the terminal.</p> <p>Normal:</p> <pre><code>[memsearch v0.1.11] embedding: openai/text-embedding-3-small | milvus: ~/.memsearch/milvus.db\n</code></pre> <p>API key missing:</p> <pre><code>[memsearch v0.1.11] embedding: openai/text-embedding-3-small | milvus: ~/.memsearch/milvus.db | ERROR: OPENAI_API_KEY not set \u2014 memory search disabled\n</code></pre> <p>Update available:</p> <pre><code>[memsearch v0.1.11] embedding: openai/text-embedding-3-small | milvus: ~/.memsearch/milvus.db | UPDATE: v0.1.12 available\n</code></pre>"},{"location":"claude-plugin/#error-key-not-set-memory-search-disabled","title":"\"ERROR: \\&lt;KEY&gt; not set -- memory search disabled\"","text":"<p>The plugin checks for the required API key at session start. If missing, memory recording still writes <code>.md</code> files, but semantic search and indexing are disabled.</p> Provider Required environment variable <code>openai</code> (default) <code>OPENAI_API_KEY</code> <code>google</code> <code>GOOGLE_API_KEY</code> <code>voyage</code> <code>VOYAGE_API_KEY</code> <code>ollama</code> None (local) <code>local</code> None (local) <p>Fix: export the key for your configured provider:</p> <pre><code># For OpenAI (default)\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Or switch to a provider that needs no key\nmemsearch config set embedding.provider ollama\n</code></pre> <p>To make it permanent, add the export to your <code>~/.bashrc</code>, <code>~/.zshrc</code>, or equivalent.</p>"},{"location":"claude-plugin/#update-v0xx-available","title":"\"UPDATE: v0.x.x available\"","text":"<p>The plugin checks PyPI at session start (2s timeout) and shows this hint when a newer version exists. How to upgrade depends on your installation method:</p> <pre><code># If installed via uv tool\nuv tool upgrade memsearch\n\n# If installed via pip\npip install --upgrade memsearch\n\n# If using uvx (auto-upgraded on each session -- you shouldn't see this)\nuvx --upgrade memsearch --version\n</code></pre> <p>Note</p> <p><code>uvx</code> users get automatic upgrades -- the plugin runs <code>uvx --upgrade</code> on every bootstrap. The <code>UPDATE</code> hint primarily helps <code>pip</code>/<code>uv tool</code> users who have no automatic update mechanism.</p>"},{"location":"claude-plugin/#2-debug-mode-debug","title":"2. Debug Mode (<code>--debug</code>)","text":"<p>Claude Code's <code>--debug</code> flag enables verbose logging for all hooks.</p> <p>Start Claude Code with debug logging:</p> <pre><code>claude --debug\n</code></pre> <p>Log location: <code>~/.claude/logs/</code> (timestamped files)</p> <p>What to look for in the logs:</p> <pre><code># See all hook outputs (additionalContext, systemMessage, etc.)\ngrep -A 5 'hook' ~/.claude/logs/*.log\n\n# Check SessionStart output specifically\ngrep -A 10 'SessionStart' ~/.claude/logs/*.log\n\n# See what additionalContext was injected\ngrep 'additionalContext' ~/.claude/logs/*.log\n</code></pre> <p>Each hook outputs JSON to stdout. In debug mode, you can see the raw JSON -- useful for verifying that <code>additionalContext</code> (cold-start memories) and <code>systemMessage</code> (status line) are being returned correctly.</p>"},{"location":"claude-plugin/#3-cli-diagnostic-commands","title":"3. CLI Diagnostic Commands","text":"<p>These commands work outside of Claude Code -- run them directly in your terminal.</p> <p>Verify resolved configuration:</p> <pre><code>memsearch config list --resolved\n</code></pre> <p>Shows the effective config after merging all layers (defaults \u2192 <code>~/.memsearch/config.toml</code> \u2192 <code>.memsearch.toml</code> \u2192 env vars). Check that <code>embedding.provider</code>, <code>embedding.model</code>, and <code>milvus.uri</code> are what you expect.</p> <p>Check index health:</p> <pre><code>memsearch stats\n</code></pre> <p>Shows collection name, chunk count, and embedding dimensions. If the count is 0 or unexpectedly low, re-index:</p> <pre><code>memsearch index .memsearch/memory/ --force\n</code></pre> <p>Test search manually:</p> <pre><code>memsearch search \"your query here\" --top-k 5\n</code></pre> <p>If this returns no results but <code>stats</code> shows chunks exist, the issue is likely with embeddings (wrong API key, different model than what was used for indexing).</p> <p>Expand a specific chunk:</p> <pre><code>memsearch expand &lt;chunk_hash&gt;\n</code></pre> <p>Retrieves the full markdown section surrounding a chunk, including session anchors. Useful for verifying that the L2 expand layer works.</p> <p>Trace back to original conversation:</p> <pre><code>memsearch transcript /path/to/session.jsonl\nmemsearch transcript /path/to/session.jsonl --turn &lt;uuid&gt; --context 3\n</code></pre> <p>Lists all turns or drills into a specific turn. The transcript path is embedded in session anchors (the <code>&lt;!-- session:... transcript:... --&gt;</code> HTML comments in memory files).</p>"},{"location":"claude-plugin/#4-watch-process","title":"4. Watch Process","text":"<p>The <code>memsearch watch</code> singleton runs in the background, auto-re-indexing when memory files change.</p> <p>PID file location: <code>.memsearch/.watch.pid</code></p> <p>Check if it's running:</p> <pre><code>cat .memsearch/.watch.pid &amp;&amp; kill -0 $(cat .memsearch/.watch.pid) 2&gt;/dev/null &amp;&amp; echo \"running\" || echo \"not running\"\n</code></pre> <p>Restart manually:</p> <pre><code># Kill existing watch (if any) and start fresh\nkill $(cat .memsearch/.watch.pid) 2&gt;/dev/null; rm -f .memsearch/.watch.pid\nmemsearch watch .memsearch/memory/ &amp;\necho $! &gt; .memsearch/.watch.pid\n</code></pre> <p>Sweep for orphaned processes:</p> <pre><code>pgrep -f \"memsearch watch\" &amp;&amp; echo \"found orphans\" || echo \"clean\"\n</code></pre> <p>The watch process is started by <code>SessionStart</code> and stopped by <code>SessionEnd</code>. If Claude Code crashes or is killed with SIGKILL, the <code>SessionEnd</code> hook won't fire and the process may become orphaned. The next <code>SessionStart</code> always stops any existing watch before starting a new one.</p> <p>Note: Milvus Lite does not support concurrent access, so the plugin falls back to one-time indexing at session start instead of a persistent watcher. For real-time indexing, use Milvus Server or Zilliz Cloud.</p>"},{"location":"claude-plugin/#5-skill-execution-progressive-disclosure","title":"5. Skill Execution &amp; Progressive Disclosure","text":"<p>When Claude decides past context is needed, it invokes the <code>memory-recall</code> skill. You can observe the three progressive disclosure layers in the Claude Code UI:</p> <pre><code>\u256d\u2500 memory-recall                                          \u2500\u256e\n\u2502                                                          \u2502\n\u2502  \u25cf Searching for relevant memories...                    \u2502\n\u2502                                                          \u2502\n\u2502  $ memsearch search \"redis caching\" --top-k 5           \u2502\n\u2502    \u2192 3 results found                                     \u2502\n\u2502                                                          \u2502\n\u2502  $ memsearch expand 7a3f9b21e4c08d56                     \u2502\n\u2502    \u2192 Full section from 2026-02-10.md                     \u2502\n\u2502                                                          \u2502\n\u2502  Summary: Found relevant context about Redis caching...  \u2502\n\u2502                                                          \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The skill runs in a forked subagent (<code>context: fork</code>), so its intermediate work does not pollute your main conversation context.</p> <p>Force a skill invocation for debugging:</p> <pre><code>/memory-recall &lt;your query&gt;\n</code></pre> <p>This manually triggers the skill, bypassing Claude's judgment about whether memory is needed.</p> <p>Skill not triggering automatically? Possible reasons:</p> <ul> <li>Claude judged that the question doesn't need historical context -- this is by design</li> <li>The <code>UserPromptSubmit</code> hint (<code>[memsearch] Memory available</code>) didn't fire -- check that the prompt is \u2265 10 characters</li> <li><code>memsearch</code> is not installed or not in PATH -- the <code>UserPromptSubmit</code> hook returns <code>{}</code> when <code>MEMSEARCH_CMD</code> is empty</li> </ul>"},{"location":"claude-plugin/#6-memory-files","title":"6. Memory Files","text":"<p>All memories are stored as plain markdown in <code>.memsearch/memory/</code>.</p> <p>Directory location: <code>.memsearch/memory/</code> (project-scoped)</p> <p>File format: One file per day, named <code>YYYY-MM-DD.md</code>:</p> <pre><code>## Session 14:30\n\n### 14:30\n&lt;!-- session:abc123def turn:ghi789jkl transcript:/home/user/.claude/projects/.../abc123def.jsonl --&gt;\n- Implemented caching system with Redis L1 and in-process LRU L2\n- Fixed N+1 query issue in order-service using selectinload\n</code></pre> <p>Verify the Stop hook is working:</p> <pre><code># Check if today's file exists and has content\ncat .memsearch/memory/$(date +%Y-%m-%d).md\n\n# Check if recent sessions have summaries (not just headings)\ntail -20 .memsearch/memory/$(date +%Y-%m-%d).md\n</code></pre> <p>If you see <code>## Session HH:MM</code> headings but no <code>### HH:MM</code> sub-headings with bullet points underneath, the Stop hook is not completing successfully. Common causes:</p> <ul> <li><code>claude</code> CLI not found -- the Stop hook calls <code>claude -p --model haiku</code> to summarize</li> <li>API key missing -- the Stop hook skips summarization when the embedding provider key is not set</li> <li>Transcript too short -- sessions with fewer than 3 JSONL lines are skipped</li> </ul>"},{"location":"claude-plugin/#common-issues","title":"Common Issues","text":"Symptom Check Section \"ERROR: \\&lt;KEY&gt; not set\" in status line Export the required API key for your provider \u00a71 \"UPDATE: v0.x.x available\" in status line Upgrade memsearch \u00a71 Search returns no results Run <code>memsearch stats</code> and <code>memsearch search</code> manually \u00a73 New memories not being indexed Check watch process is running \u00a74 Claude never invokes memory recall Try <code>/memory-recall &lt;query&gt;</code> manually \u00a75 Session summaries missing from memory files Check <code>claude</code> CLI is available and API key is set \u00a76"},{"location":"cli/","title":"CLI Reference","text":"<p>memsearch provides a command-line interface for indexing, searching, and managing semantic memory over markdown knowledge bases.</p> <pre><code>$ memsearch --version\nmemsearch, version 0.1.3\n\n$ memsearch --help\nUsage: memsearch [OPTIONS] COMMAND [ARGS]...\n\n  memsearch \u2014 semantic memory search for markdown knowledge bases.\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  compact     Compress stored memories into a summary.\n  config      Manage memsearch configuration.\n  expand      Expand a memory chunk to show full context.\n  index       Index markdown files from PATHS.\n  reset       Drop all indexed data.\n  search      Search indexed memory for QUERY.\n  stats       Show statistics about the index.\n  transcript  View original conversation turns from a JSONL transcript.\n  watch       Watch PATHS for markdown changes and auto-index.\n</code></pre>"},{"location":"cli/#command-summary","title":"Command Summary","text":"Command Description <code>memsearch config</code> Initialize, view, and modify configuration <code>memsearch index</code> Scan directories and index markdown files into the vector store <code>memsearch search</code> Semantic search across indexed chunks using natural language <code>memsearch watch</code> Monitor directories and auto-index on file changes <code>memsearch compact</code> Compress indexed chunks into an LLM-generated summary <code>memsearch expand</code> Progressive disclosure L2: show full section around a chunk \ud83d\udd0c <code>memsearch transcript</code> Progressive disclosure L3: view turns from a JSONL transcript \ud83d\udd0c <code>memsearch stats</code> Display index statistics (total chunk count) <code>memsearch reset</code> Drop all indexed data from the Milvus collection <p>\ud83d\udd0c Commands marked with \ud83d\udd0c are designed for the Claude Code plugin's progressive disclosure workflow, but work as standalone CLI tools too.</p>"},{"location":"cli/#memsearch-config","title":"<code>memsearch config</code>","text":"<p>Manage memsearch configuration. Configuration is stored in TOML files and follows a layered priority chain:</p> <pre><code>dataclass defaults -&gt; ~/.memsearch/config.toml -&gt; .memsearch.toml -&gt; CLI flags\n</code></pre> <p>Higher-priority sources override lower-priority ones.</p>"},{"location":"cli/#subcommands","title":"Subcommands","text":""},{"location":"cli/#memsearch-config-init","title":"<code>memsearch config init</code>","text":"<p>Launch an interactive wizard that walks through all configuration sections and writes a TOML config file.</p> Flag Default Description <code>--project</code> <code>false</code> Write to <code>.memsearch.toml</code> (project-level) instead of the global config <pre><code>$ memsearch config init\nmemsearch configuration wizard\nWriting to: /home/user/.memsearch/config.toml\n\n-- Milvus --\n  Milvus URI [~/.memsearch/milvus.db]:\n  Milvus token (empty for none) []:\n  Collection name [memsearch_chunks]:\n\n-- Embedding --\n  Provider (openai/google/voyage/ollama/local) [openai]:\n  Model (empty for provider default) []:\n\n-- Chunking --\n  Max chunk size (chars) [1500]:\n  Overlap lines [2]:\n\n-- Watch --\n  Debounce (ms) [1500]:\n\n-- Compact --\n  LLM provider [openai]:\n  LLM model (empty for default) []:\n  Prompt file path (empty for built-in) []:\n\nConfig saved to /home/user/.memsearch/config.toml\n</code></pre> <p>Create a project-level config:</p> <pre><code>$ memsearch config init --project\nmemsearch configuration wizard\nWriting to: .memsearch.toml\n...\n</code></pre>"},{"location":"cli/#memsearch-config-set","title":"<code>memsearch config set</code>","text":"<p>Set a single configuration value by dotted key. Keys follow the <code>section.field</code> format.</p> Flag Default Description <code>KEY</code> (required) Dotted config key (e.g., <code>milvus.uri</code>) <code>VALUE</code> (required) Value to set <code>--project</code> <code>false</code> Write to <code>.memsearch.toml</code> instead of global config <pre><code>$ memsearch config set milvus.uri http://localhost:19530\nSet milvus.uri = http://localhost:19530 in /home/user/.memsearch/config.toml\n\n$ memsearch config set embedding.provider google --project\nSet embedding.provider = google in .memsearch.toml\n\n$ memsearch config set chunking.max_chunk_size 2000\nSet chunking.max_chunk_size = 2000 in /home/user/.memsearch/config.toml\n</code></pre>"},{"location":"cli/#memsearch-config-get","title":"<code>memsearch config get</code>","text":"<p>Read a single resolved configuration value (merged from all sources).</p> <pre><code>$ memsearch config get milvus.uri\nhttp://localhost:19530\n\n$ memsearch config get embedding.provider\nopenai\n\n$ memsearch config get chunking.max_chunk_size\n1500\n</code></pre>"},{"location":"cli/#memsearch-config-list","title":"<code>memsearch config list</code>","text":"<p>Display configuration in TOML format.</p> Flag Default Description <code>--resolved</code> (default) Show the fully merged configuration from all sources <code>--global</code> Show only the global config file (<code>~/.memsearch/config.toml</code>) <code>--project</code> Show only the project config file (<code>.memsearch.toml</code>) <pre><code>$ memsearch config list --resolved\n# Resolved (all sources merged)\n\n[milvus]\nuri = \"~/.memsearch/milvus.db\"\ntoken = \"\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"\nprompt_file = \"\"\n</code></pre> <pre><code>$ memsearch config list --global\n# Global (/home/user/.memsearch/config.toml)\n\n[milvus]\nuri = \"http://localhost:19530\"\n\n[embedding]\nprovider = \"openai\"\n</code></pre>"},{"location":"cli/#available-config-keys","title":"Available Config Keys","text":"Key Type Default Description <code>milvus.uri</code> string <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>milvus.token</code> string <code>\"\"</code> Auth token for Milvus Server / Zilliz Cloud <code>milvus.collection</code> string <code>memsearch_chunks</code> Collection name <code>embedding.provider</code> string <code>openai</code> Embedding provider name <code>embedding.model</code> string <code>\"\"</code> Override embedding model (empty = provider default) <code>chunking.max_chunk_size</code> int <code>1500</code> Maximum chunk size in characters <code>chunking.overlap_lines</code> int <code>2</code> Number of overlapping lines between adjacent chunks <code>watch.debounce_ms</code> int <code>1500</code> File watcher debounce delay in milliseconds <code>compact.llm_provider</code> string <code>openai</code> LLM provider for compact summarization <code>compact.llm_model</code> string <code>\"\"</code> Override LLM model (empty = provider default) <code>compact.prompt_file</code> string <code>\"\"</code> Path to a custom prompt template file"},{"location":"cli/#memsearch-index","title":"<code>memsearch index</code>","text":"<p>Scan one or more directories (or files) and index all markdown files (<code>.md</code>, <code>.markdown</code>) into the Milvus vector store. Only new or changed chunks are embedded by default -- unchanged chunks are skipped. Chunks belonging to deleted files are automatically removed from the index.</p>"},{"location":"cli/#options","title":"Options","text":"Flag Short Default Description <code>PATHS</code> (required) One or more directories or files to index <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (<code>openai</code>, <code>google</code>, <code>voyage</code>, <code>ollama</code>, <code>local</code>) <code>--model</code> <code>-m</code> provider default Override the embedding model name <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token (for server or Zilliz Cloud) <code>--force</code> <code>false</code> Re-embed and re-index all chunks, even if unchanged"},{"location":"cli/#examples","title":"Examples","text":"<p>Index a single directory:</p> <pre><code>$ memsearch index ./memory/\nIndexed 42 chunks.\n</code></pre> <p>Index multiple directories with a specific embedding provider:</p> <pre><code>$ memsearch index ./memory/ ./notes/ --provider google\nIndexed 87 chunks.\n</code></pre> <p>Force re-index everything (ignores the content-hash dedup check):</p> <pre><code>$ memsearch index ./memory/ --force\nIndexed 42 chunks.\n</code></pre> <p>Connect to a remote Milvus server instead of the default local file:</p> <pre><code>$ memsearch index ./memory/ --milvus-uri http://localhost:19530\nIndexed 42 chunks.\n</code></pre> <p>Use a custom embedding model:</p> <pre><code>$ memsearch index ./memory/ --provider openai --model text-embedding-3-large\nIndexed 42 chunks.\n</code></pre>"},{"location":"cli/#notes","title":"Notes","text":"<ul> <li>Incremental by default. Each chunk is identified by a composite hash of its source file, line range, content hash, and embedding model. Only chunks with new IDs are embedded and stored.</li> <li>Stale cleanup. If a file that was previously indexed no longer exists on disk, its chunks are automatically deleted from the index during the next <code>index</code> run.</li> <li><code>--force</code> re-embeds everything. Use this when you switch embedding providers or models, since the same content will produce different vectors with a different model.</li> </ul>"},{"location":"cli/#memsearch-search","title":"<code>memsearch search</code>","text":"<p>Run a semantic search query against indexed chunks. Uses hybrid search (dense vector cosine similarity + BM25 full-text) with RRF reranking for best results.</p>"},{"location":"cli/#options_1","title":"Options","text":"Flag Short Default Description <code>QUERY</code> (required) Natural-language search query <code>--top-k</code> <code>-k</code> <code>5</code> Maximum number of results to return <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (must match the provider used at index time) <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token <code>--json-output</code> <code>-j</code> <code>false</code> Output results as JSON"},{"location":"cli/#examples_1","title":"Examples","text":"<p>Basic search:</p> <pre><code>$ memsearch search \"how to configure Redis caching\"\n\n--- Result 1 (score: 0.0328) ---\nSource: /home/user/docs/2026-01-15.md\nHeading: Redis Configuration\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry...\n\n--- Result 2 (score: 0.0326) ---\nSource: /home/user/docs/architecture.md\nHeading: Caching Layer\nWe use Redis as the primary caching backend...\n</code></pre> <p>Return more results:</p> <pre><code>$ memsearch search \"authentication flow\" --top-k 10\n</code></pre> <p>Output as JSON (useful for piping to <code>jq</code> or other tools):</p> <pre><code>$ memsearch search \"error handling\" --json-output\n[\n  {\n    \"content\": \"All API endpoints should return structured error...\",\n    \"source\": \"/home/user/docs/api-design.md\",\n    \"heading\": \"Error Handling\",\n    \"chunk_hash\": \"a1b2c3d4e5f6...\",\n    \"heading_level\": 2,\n    \"start_line\": 45,\n    \"end_line\": 62,\n    \"score\": 0.0330\n  }\n]\n</code></pre> <p>Use with a different provider (must match the one used for indexing):</p> <pre><code>$ memsearch search \"database migrations\" --provider google\n</code></pre>"},{"location":"cli/#notes_1","title":"Notes","text":"<ul> <li>Provider must match. The search embedding provider and model must match whatever was used during indexing. Mixing providers will return poor results because the vector spaces are incompatible.</li> <li>Hybrid search. Results are ranked using Reciprocal Rank Fusion (RRF) across both dense (cosine) and sparse (BM25) retrieval, giving you the best of semantic and keyword matching.</li> <li>Content is truncated. In the default text output, each result's content is truncated to 500 characters. Use <code>--json-output</code> to get the full content.</li> </ul>"},{"location":"cli/#memsearch-watch","title":"<code>memsearch watch</code>","text":"<p>Start a long-running file watcher that monitors directories for markdown file changes. On startup, all existing markdown files are indexed first (dedup ensures no wasted API calls for unchanged content). Then the watcher monitors for changes: when a <code>.md</code> or <code>.markdown</code> file is created or modified, it is automatically re-indexed. When a file is deleted, its chunks are removed from the store.</p>"},{"location":"cli/#options_2","title":"Options","text":"Flag Short Default Description <code>PATHS</code> (required) One or more directories to watch <code>--debounce-ms</code> <code>1500</code> Debounce delay in milliseconds; multiple rapid changes to the same file within this window are collapsed into a single re-index <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_2","title":"Examples","text":"<p>Watch a single directory:</p> <pre><code>$ memsearch watch ./memory/\nIndexed 8 chunks.\nWatching 1 path(s) for changes... (Ctrl+C to stop)\nIndexed 3 chunks from /home/user/docs/2026-02-11.md\nRemoved chunks for /home/user/docs/old-draft.md\n^C\nStopping watcher.\n</code></pre> <p>Watch multiple directories with a longer debounce:</p> <pre><code>$ memsearch watch ./memory/ ./notes/ --debounce-ms 3000\nWatching 2 path(s) for changes... (Ctrl+C to stop)\n</code></pre>"},{"location":"cli/#notes_2","title":"Notes","text":"<ul> <li>Initial index on startup. The watcher indexes all existing files before it starts monitoring. Content-hash dedup means unchanged files are skipped with zero API calls \u2014 only genuinely new or modified content is embedded.</li> <li>Debounce. Editors that write files in multiple steps (e.g., write temp file, then rename) can trigger several events in quick succession. The debounce window collapses these into one re-index operation.</li> <li>Recursive. The watcher monitors all subdirectories recursively.</li> <li>Singleton behavior. Only one watcher process should run per directory set. Running multiple watchers on the same paths will cause duplicate indexing work (though dedup by content hash means the index stays consistent).</li> <li>Stop with Ctrl+C. The watcher runs until you interrupt it.</li> </ul>"},{"location":"cli/#memsearch-compact","title":"<code>memsearch compact</code>","text":"<p>Use an LLM to compress all indexed chunks (or a subset) into a condensed markdown summary. The summary is appended to a daily log file at <code>memory/YYYY-MM-DD.md</code> inside the first configured path, keeping markdown as the single source of truth.</p>"},{"location":"cli/#options_3","title":"Options","text":"Flag Short Default Description <code>--source</code> <code>-s</code> (all chunks) Only compact chunks from this specific source file <code>--output-dir</code> <code>-o</code> first configured path Directory to write the compact summary into <code>--llm-provider</code> <code>openai</code> LLM backend for summarization (<code>openai</code>, <code>anthropic</code>, <code>gemini</code>) <code>--llm-model</code> provider default Override the LLM model <code>--prompt</code> built-in template Custom prompt template string (must contain <code>{chunks}</code> placeholder) <code>--prompt-file</code> (none) Read the prompt template from a file instead <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (used to access the index) <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#default-llm-models","title":"Default LLM Models","text":"Provider Default Model <code>openai</code> <code>gpt-4o-mini</code> <code>anthropic</code> <code>claude-sonnet-4-5-20250929</code> <code>gemini</code> <code>gemini-2.0-flash</code>"},{"location":"cli/#examples_3","title":"Examples","text":"<p>Compact all chunks using the default LLM (OpenAI):</p> <pre><code>$ memsearch compact\nCompact complete. Summary:\n\n## Key Decisions\n- Use Redis for session caching with 5-minute TTL\n- All API errors return structured JSON responses\n...\n</code></pre> <p>Compact only chunks from a specific source file:</p> <pre><code>$ memsearch compact --source ./memory/old-notes.md\nCompact complete. Summary:\n\n## Old Notes Summary\n- Initial architecture decisions from January meeting...\n</code></pre> <p>Use Anthropic Claude for summarization:</p> <pre><code>$ memsearch compact --llm-provider anthropic\n</code></pre> <p>Use a custom prompt template:</p> <pre><code>$ memsearch compact --prompt \"Summarize these notes into action items:\\n{chunks}\"\n</code></pre> <p>Use a prompt file for complex templates:</p> <pre><code>$ memsearch compact --prompt-file ./prompts/compress.txt\n</code></pre>"},{"location":"cli/#notes_3","title":"Notes","text":"<ul> <li>Output location. The summary is appended to <code>&lt;first-path&gt;/memory/YYYY-MM-DD.md</code>. This file is then automatically eligible for future indexing.</li> <li>The <code>{chunks}</code> placeholder is required. Whether using <code>--prompt</code> or <code>--prompt-file</code>, the template must contain <code>{chunks}</code> which will be replaced with the concatenated chunk contents.</li> <li>API key required. The chosen LLM provider requires its corresponding API key in the environment (see Environment Variables).</li> </ul>"},{"location":"cli/#memsearch-expand","title":"<code>memsearch expand</code>","text":"<p>\ud83d\udd0c Claude Code plugin command. This command is part of the Claude Code plugin's three-level progressive disclosure workflow (<code>search</code> \u2192 <code>expand</code> \u2192 <code>transcript</code>), but works as a standalone CLI tool for any memsearch index.</p> <p>Look up a chunk by its hash in the index and return the surrounding context from the original source markdown file. This is \"progressive disclosure level 2\" -- when a search result snippet is not enough, expand it to see the full heading section.</p>"},{"location":"cli/#options_4","title":"Options","text":"Flag Short Default Description <code>CHUNK_HASH</code> (required) The chunk hash (primary key) to look up <code>--section/--no-section</code> <code>--section</code> Show the full heading section (default behavior) <code>--lines</code> <code>-n</code> (full section) Instead of the full section, show N lines before and after the chunk <code>--json-output</code> <code>-j</code> <code>false</code> Output as JSON <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_4","title":"Examples","text":"<p>Expand a chunk to see its full heading section:</p> <pre><code>$ memsearch expand a1b2c3d4e5f6\nSource: /home/user/docs/architecture.md (lines 10-35)\nHeading: Caching Layer\n\n## Caching Layer\n\nWe use Redis as the primary caching backend. All cache keys\nfollow the pattern `service:entity:id`.\n\n### Configuration\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry.\n...\n</code></pre> <p>Show only 5 lines of context around the chunk:</p> <pre><code>$ memsearch expand a1b2c3d4e5f6 --lines 5\nSource: /home/user/docs/architecture.md (lines 18-28)\nHeading: Caching Layer\n\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry.\n...\n</code></pre> <p>Get JSON output (includes anchor metadata if present):</p> <pre><code>$ memsearch expand a1b2c3d4e5f6 --json-output\n{\n  \"chunk_hash\": \"a1b2c3d4e5f6\",\n  \"source\": \"/home/user/docs/architecture.md\",\n  \"heading\": \"Caching Layer\",\n  \"start_line\": 10,\n  \"end_line\": 35,\n  \"content\": \"## Caching Layer\\n\\nWe use Redis as the primary...\"\n}\n</code></pre>"},{"location":"cli/#notes_4","title":"Notes","text":"<ul> <li>Source file must exist. The <code>expand</code> command reads the original markdown file from disk. If the source file has been moved or deleted, the command will fail with an error.</li> <li>Anchor parsing. If the expanded text contains an HTML anchor comment in the format <code>&lt;!-- session:ID turn:ID transcript:PATH --&gt;</code>, the command parses it and displays the session, turn, and transcript file information. This connects memory chunks to their original conversation transcripts.</li> <li>Workflow: search then expand. A typical workflow is to <code>search</code> first, note the <code>chunk_hash</code> from a result, then <code>expand</code> it to see more context.</li> </ul>"},{"location":"cli/#memsearch-transcript","title":"<code>memsearch transcript</code>","text":"<p>\ud83d\udd0c Claude Code plugin command. This command is part of the Claude Code plugin's three-level progressive disclosure workflow (<code>search</code> \u2192 <code>expand</code> \u2192 <code>transcript</code>), but works as a standalone CLI tool for any JSONL transcript.</p> <p>Parse a JSONL transcript file (e.g., from Claude Code) and display conversation turns. This is \"progressive disclosure level 3\" -- drill all the way down from a memory chunk to the original conversation that generated it.</p>"},{"location":"cli/#options_5","title":"Options","text":"Flag Short Default Description <code>JSONL_PATH</code> (required) Path to the JSONL transcript file <code>--turn</code> <code>-t</code> (show all) Target turn UUID (prefix match supported) <code>--context</code> <code>-c</code> <code>3</code> Number of turns to show before and after the target turn <code>--json-output</code> <code>-j</code> <code>false</code> Output as JSON"},{"location":"cli/#examples_5","title":"Examples","text":"<p>List all turns in a transcript:</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl\nAll turns (12):\n\n  a1b2c3d4e5f6  14:23:05  Show me the Redis configuration code\n  d4e5f6a1b2c3  14:23:42  Can you add TTL support to the cache?\n  f6a1b2c3d4e5  14:25:10  Write tests for the cache module\n  ...\n</code></pre> <p>Show context around a specific turn (prefix match on UUID):</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl --turn d4e5f6\nShowing 5 turns around d4e5f6a1b2c3:\n\n[14:22:30] a1b2c3d4\nShow me the Redis configuration code\n\n**Assistant**: Here is the current Redis configuration...\n\n&gt;&gt;&gt; [14:23:42] d4e5f6a1\nCan you add TTL support to the cache?\n\n**Assistant**: I'll add TTL support. Here are the changes...\n  Tools: Edit(/src/cache.py), Bash(pytest tests/)\n\n[14:25:10] f6a1b2c3\nWrite tests for the cache module\n</code></pre> <p>Output as JSON:</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl --turn d4e5f6 --json-output\n[\n  {\n    \"uuid\": \"a1b2c3d4-...\",\n    \"timestamp\": \"2026-02-10T14:22:30Z\",\n    \"content\": \"Show me the Redis configuration code\\n\\n**Assistant**: ...\",\n    \"tool_calls\": []\n  }\n]\n</code></pre>"},{"location":"cli/#notes_5","title":"Notes","text":"<ul> <li>UUID prefix matching. You do not need to provide the full UUID. The first 6-8 characters are usually enough to uniquely identify a turn.</li> <li>The <code>&gt;&gt;&gt;</code> marker in text output highlights the target turn when using <code>--turn</code>.</li> <li>Three-level progressive disclosure workflow: <code>search</code> (L1: chunk snippet) -&gt; <code>expand</code> (L2: full section) -&gt; <code>transcript</code> (L3: original conversation).</li> </ul>"},{"location":"cli/#memsearch-stats","title":"<code>memsearch stats</code>","text":"<p>Show statistics about the current index, including the total number of stored chunks.</p>"},{"location":"cli/#options_6","title":"Options","text":"Flag Short Default Description <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_6","title":"Examples","text":"<pre><code>$ memsearch stats\nTotal indexed chunks: 142\n</code></pre> <p>Check stats for a specific collection on a remote server:</p> <pre><code>$ memsearch stats --milvus-uri http://localhost:19530 --collection my_project\nTotal indexed chunks: 87\n</code></pre>"},{"location":"cli/#notes_6","title":"Notes","text":"<ul> <li>Stats may lag on remote Milvus Server. The <code>get_collection_stats()</code> API on a remote Milvus Server may return stale counts immediately after an upsert. Stats are updated after segment flush and compaction. Search results are always up to date.</li> </ul>"},{"location":"cli/#memsearch-reset","title":"<code>memsearch reset</code>","text":"<p>Drop the entire Milvus collection, permanently deleting all indexed chunks. A confirmation prompt is shown before proceeding.</p>"},{"location":"cli/#options_7","title":"Options","text":"Flag Short Default Description <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token <code>--yes</code> <code>-y</code> Skip the confirmation prompt"},{"location":"cli/#examples_7","title":"Examples","text":"<pre><code>$ memsearch reset\nThis will delete all indexed data. Continue? [y/N]: y\nDropped collection.\n</code></pre> <p>Skip the confirmation prompt (useful in scripts):</p> <pre><code>$ memsearch reset --yes\nDropped collection.\n</code></pre> <p>Reset a specific collection on a remote server:</p> <pre><code>$ memsearch reset --milvus-uri http://localhost:19530 --collection old_project --yes\nDropped collection.\n</code></pre>"},{"location":"cli/#notes_7","title":"Notes","text":"<ul> <li>This is destructive and irreversible. All indexed data will be lost. Your original markdown files are not affected -- you can always re-index them with <code>memsearch index</code>.</li> <li>Only drops the collection, not the database. If you are using Milvus Lite (a local <code>.db</code> file), the file itself remains; only the collection inside it is removed.</li> </ul>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>memsearch reads API keys from environment variables. These are required by the corresponding embedding and LLM provider SDKs and are not stored in memsearch config files.</p>"},{"location":"cli/#api-keys","title":"API Keys","text":"Variable Required By Description <code>OPENAI_API_KEY</code> <code>openai</code> embedding provider, <code>openai</code> LLM compact provider OpenAI API key <code>OPENAI_BASE_URL</code> (optional) Override the OpenAI API base URL (for proxies or compatible APIs) <code>GOOGLE_API_KEY</code> <code>google</code> embedding provider, <code>gemini</code> LLM compact provider Google AI API key <code>VOYAGE_API_KEY</code> <code>voyage</code> embedding provider Voyage AI API key <code>ANTHROPIC_API_KEY</code> <code>anthropic</code> LLM compact provider Anthropic API key <code>OLLAMA_HOST</code> <code>ollama</code> embedding provider (optional) Ollama server URL (default: <code>http://localhost:11434</code>) <p>All memsearch settings (Milvus URI, embedding provider, chunking parameters, etc.) are configured via TOML config files or CLI flags -- see Configuration for details.</p>"},{"location":"cli/#examples_8","title":"Examples","text":"<pre><code># Set API key and run a search\n$ export OPENAI_API_KEY=sk-...\n$ memsearch search \"database schema\"\n\n# Use Google for embedding, Anthropic for compact\n$ export GOOGLE_API_KEY=AIza...\n$ memsearch index ./memory/ --provider google\n$ memsearch compact --llm-provider anthropic\n</code></pre>"},{"location":"cli/#embedding-provider-reference","title":"Embedding Provider Reference","text":"Provider Install Default Model Dimension API Key Variable <code>openai</code> included by default <code>text-embedding-3-small</code> 1536 <code>OPENAI_API_KEY</code> <code>google</code> <code>pip install \"memsearch[google]\"</code> <code>gemini-embedding-001</code> 768 <code>GOOGLE_API_KEY</code> <code>voyage</code> <code>pip install \"memsearch[voyage]\"</code> <code>voyage-3-lite</code> 512 <code>VOYAGE_API_KEY</code> <code>ollama</code> <code>pip install \"memsearch[ollama]\"</code> <code>nomic-embed-text</code> 768 (none, local) <code>local</code> <code>pip install \"memsearch[local]\"</code> <code>all-MiniLM-L6-v2</code> 384 (none, local) <p>Install all optional providers at once:</p> <pre><code>$ pip install \"memsearch[all]\"\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#can-i-use-memsearch-for-per-user-memory-in-a-consumer-facing-app","title":"Can I use memsearch for per-user memory in a consumer-facing app?","text":"<p>Yes. memsearch is not locked to a \"per-project\" or \"per-agent\" model. The <code>paths</code>, <code>collection</code>, and <code>milvus_uri</code> parameters can all be set dynamically per user, giving you full per-user isolation.</p> <p>Option 1 \u2014 Directory + collection isolation (recommended):</p> <pre><code>from memsearch import MemSearch\n\ndef get_user_memory(user_id: str) -&gt; MemSearch:\n    return MemSearch(\n        paths=[f\"./memory/{user_id}\"],\n        collection=f\"mem_{user_id}\",\n    )\n\n# Fully isolated \u2014 different markdown directories, different Milvus collections\nmem_alice = get_user_memory(\"alice\")\nmem_bob = get_user_memory(\"bob\")\n</code></pre> <p>Each user's memories live in their own directory and their own collection. They never see each other's data.</p> <p>Option 2 \u2014 Separate Milvus Lite databases (strongest isolation):</p> <pre><code>def get_user_memory(user_id: str) -&gt; MemSearch:\n    return MemSearch(\n        paths=[f\"./memory/{user_id}\"],\n        milvus_uri=f\"./data/{user_id}.db\",\n    )\n</code></pre> <p>Each user gets a physically separate database file. This is the simplest model when you don't need cross-user search.</p> <p>The Claude Code plugin uses per-project isolation \u2014 each project automatically gets its own Milvus collection (e.g. <code>ms_my_app_a1b2c3d4</code>) derived from the project path, so searches never leak across projects. But the underlying memsearch library has no such constraint \u2014 a consumer chat app can instantiate one <code>MemSearch</code> per user and get clean isolation.</p>"},{"location":"faq/#how-do-multiple-developers-share-memory-on-the-same-project","title":"How do multiple developers share memory on the same project?","text":"<p>Short answer: they don't need to, and usually shouldn't.</p> <p>In a typical multi-developer workflow, each person clones the repo locally and runs their own Claude Code sessions. The plugin stores memory in <code>.memsearch/memory/YYYY-MM-DD.md</code> files \u2014 these are personal session logs generated from each developer's own conversations. They are local by nature and do not need to be pushed to the shared remote.</p> <p>Here is how we think about it in our own team:</p> What Scope Version-controlled? Example Project conventions Shared across team Yes \u2014 commit to git <code>CLAUDE.md</code> (coding standards, architecture decisions, team agreements) Session memories Personal to each developer No \u2014 add to <code>.gitignore</code> <code>.memsearch/memory/2026-02-10.md</code> (what you worked on today) <pre><code># .gitignore\n.memsearch/\n</code></pre> <p>Why this works:</p> <ul> <li>No merge conflicts. Each developer's memory files only exist on their own machine. There is nothing to merge.</li> <li>No noise. Your colleagues don't need to know that you spent 45 minutes debugging a typo. Your session logs are yours.</li> <li>Shared knowledge goes in <code>CLAUDE.md</code>. Decisions that the whole team should know about (e.g., \"we use Redis for caching\", \"never use <code>SELECT *</code>\") belong in <code>CLAUDE.md</code> or a shared docs directory \u2014 version-controlled, reviewed via PR, the normal git workflow.</li> </ul> <p>If your team does want to share certain memories (e.g., onboarding notes, architecture decisions), you can put those in a shared directory that is tracked by git, and keep personal session logs in <code>.memsearch/</code> which is gitignored. memsearch can index multiple paths, so you can point it at both:</p> <pre><code>mem = MemSearch(paths=[\"./docs/shared-knowledge\", \"./.memsearch/memory\"])\n</code></pre>"},{"location":"faq/#how-does-memsearch-avoid-re-embedding-unchanged-content","title":"How does memsearch avoid re-embedding unchanged content?","text":"<p>Embedding API calls are the main cost of running a semantic search system. memsearch uses content-addressable hashing to ensure you never pay to embed the same content twice.</p> <p>Here is the mechanism:</p> <ol> <li> <p>Chunk the file. Each markdown file is split into chunks by heading structure (h1/h2/h3 boundaries).</p> </li> <li> <p>Hash each chunk. The content of each chunk is hashed with SHA-256 (truncated to 16 hex characters). This hash is combined with the source path, line range, and embedding model name to produce a composite chunk ID.</p> </li> <li> <p>Check against Milvus. Before calling the embedding API, memsearch queries Milvus for all existing chunk IDs belonging to that source file.</p> </li> <li> <p>Skip unchanged chunks. If a chunk's composite ID already exists in Milvus, it is skipped entirely \u2014 no embedding API call, no upsert.</p> </li> <li> <p>Delete stale chunks. If a chunk ID that used to exist no longer appears in the re-chunked file, it is deleted from Milvus (the content was removed or changed).</p> </li> </ol> <pre><code>file changed \u2192 re-chunk \u2192 hash each chunk \u2192 diff against Milvus\n                                                  \u2502\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502             \u2502             \u2502\n                              ID exists      ID is new    ID disappeared\n                              (skip)      (embed + upsert)  (delete)\n</code></pre> <p>In practice, this means:</p> <ul> <li>Re-indexing an unchanged knowledge base costs zero API calls. The hashes match, everything is skipped.</li> <li>Editing one section of a file only re-embeds that section's chunks. The rest of the file is untouched.</li> <li>The file watcher (<code>memsearch watch</code>) uses this same mechanism. When it detects a file change, it re-chunks and re-hashes \u2014 only the actually-changed chunks hit the embedding API.</li> </ul> <p>For more details on the hashing scheme and storage architecture, see the Architecture \u2014 Deduplication page.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install memsearch with pip (OpenAI embeddings are included by default):</p> <pre><code>$ pip install memsearch\n</code></pre>"},{"location":"getting-started/#extras-for-additional-embedding-providers","title":"Extras for additional embedding providers","text":"<p>Each optional extra pulls in the provider SDK you need:</p> <pre><code>$ pip install \"memsearch[google]\"      # Google Gemini embeddings\n$ pip install \"memsearch[voyage]\"      # Voyage AI embeddings\n$ pip install \"memsearch[ollama]\"      # Ollama (local, no API key)\n$ pip install \"memsearch[local]\"       # sentence-transformers (local, no API key)\n$ pip install \"memsearch[anthropic]\"   # Anthropic (for compact/summarization LLM)\n$ pip install \"memsearch[all]\"         # Everything above\n</code></pre>"},{"location":"getting-started/#how-it-all-fits-together","title":"How It All Fits Together","text":"<p>The diagram below shows the full lifecycle: writing markdown, indexing chunks, and searching them later.</p> <pre><code>sequenceDiagram\n    participant U as Your App\n    participant M as MemSearch\n    participant E as Embedding API\n    participant V as Milvus\n\n    U-&gt;&gt;M: save_memory(\"Redis config...\")\n    U-&gt;&gt;M: mem.index()\n    M-&gt;&gt;M: Chunk markdown\n    M-&gt;&gt;M: SHA-256 dedup\n    M-&gt;&gt;E: Embed new chunks\n    E--&gt;&gt;M: Vectors\n    M-&gt;&gt;V: Upsert\n    U-&gt;&gt;M: mem.search(\"Redis?\")\n    M-&gt;&gt;E: Embed query\n    E--&gt;&gt;M: Query vector\n    M-&gt;&gt;V: Cosine similarity\n    V--&gt;&gt;M: Top-K matches\n    M--&gt;&gt;U: Results with source info</code></pre> <p>Markdown is the source of truth. The vector store is a derived index -- rebuildable anytime from the original <code>.md</code> files. This means your memory is human-readable, <code>git</code>-friendly, and never locked into a proprietary format.</p>"},{"location":"getting-started/#your-first-memory-search","title":"Your First Memory Search","text":"<p>This section walks through the complete flow: create a memory directory, write some markdown files, index them, and search.</p>"},{"location":"getting-started/#set-up-your-memory-directory","title":"Set up your memory directory","text":"<p>memsearch follows the OpenClaw memory layout: a <code>MEMORY.md</code> file for persistent facts, plus daily logs in a <code>memory/</code> subdirectory.</p> <pre><code>$ mkdir -p my-project/memory\n$ cd my-project\n</code></pre> <p>Write a <code>MEMORY.md</code> with long-lived facts:</p> <pre><code>$ cat &gt; MEMORY.md &lt;&lt; 'EOF'\n# MEMORY.md\n\n## Team\n- Alice: frontend lead, React expert\n- Bob: backend lead, Python/FastAPI\n- Charlie: DevOps, manages Kubernetes\n\n## Architecture Decisions\n- ADR-001: Use event-driven architecture with Kafka\n- ADR-002: PostgreSQL 16 as primary database\n- ADR-003: Redis 7 for caching and sessions\n- ADR-004: Milvus for product semantic search\nEOF\n</code></pre> <p>Write a daily log:</p> <pre><code>$ cat &gt; memory/2026-02-10.md &lt;&lt; 'EOF'\n# 2026-02-10\n\n## Standup Notes\n- Alice finished the checkout redesign, merging today\n- Bob fixed the N+1 query in the order service \u2014 response time dropped from 800ms to 120ms\n- Charlie set up staging auto-deploy via GitHub Actions\n\n## Decision\nWe decided to migrate from REST to gRPC for inter-service communication.\nThe main drivers: type safety, streaming support, and ~40% latency reduction in benchmarks.\nEOF\n</code></pre>"},{"location":"getting-started/#index-with-the-cli","title":"Index with the CLI","text":"<pre><code>$ export OPENAI_API_KEY=\"sk-...\"\n$ memsearch index .\nIndexed 8 chunks.\n</code></pre>"},{"location":"getting-started/#search-with-the-cli","title":"Search with the CLI","text":"<pre><code>$ memsearch search \"what caching solution are we using?\"\n--- Result 1 (score: 0.0332) ---\nSource: MEMORY.md\nHeading: Architecture Decisions\n- ADR-003: Redis 7 for caching and sessions\n\n$ memsearch search \"what did Bob work on recently?\" --top-k 3\n--- Result 1 (score: 0.0328) ---\nSource: memory/2026-02-10.md\nHeading: Standup Notes\n- Bob fixed the N+1 query in the order service \u2014 response time dropped from 800ms to 120ms\n</code></pre> <p>Use <code>--json-output</code> to get structured results for piping into other tools:</p> <pre><code>$ memsearch search \"inter-service communication\" --json-output | python -m json.tool\n</code></pre>"},{"location":"getting-started/#search-with-the-python-api","title":"Search with the Python API","text":"<p>The same workflow in Python:</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\n\nasync def main():\n    mem = MemSearch(paths=[\".\"])\n    await mem.index()\n\n    results = await mem.search(\"what caching solution are we using?\", top_k=3)\n    for r in results:\n        print(f\"[{r['score']:.4f}] {r['source']} \u2014 {r['heading']}\")\n        print(f\"  {r['content'][:200]}\\n\")\n\n    mem.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#building-an-agent-with-memory","title":"Building an Agent with Memory","text":"<p>The real power of memsearch is giving an LLM agent persistent memory across conversations. The pattern is simple: recall, think, remember.</p> <ol> <li>Recall -- search past memories for context relevant to the user's question</li> <li>Think -- call the LLM with that context injected into the system prompt</li> <li>Remember -- save the exchange to a daily markdown log and re-index</li> </ol>"},{"location":"getting-started/#openai-example-default","title":"OpenAI example (default)","text":"<pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = OpenAI()\nmem = MemSearch(paths=[MEMORY_DIR])\n\n\ndef save_memory(content: str):\n    \"\"\"Append a note to today's memory log (OpenClaw-style daily markdown).\"\"\"\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        if p.stat().st_size == 0:\n            f.write(f\"# {date.today()}\\n\")\n        f.write(f\"\\n{content}\\n\")\n\n\nasync def agent_chat(user_input: str) -&gt; str:\n    # 1. Recall \u2014 search past memories for relevant context\n    memories = await mem.search(user_input, top_k=5)\n    context = \"\\n\".join(f\"- {m['content'][:300]}\" for m in memories)\n\n    # 2. Think \u2014 call LLM with memory context\n    resp = llm.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a helpful assistant with access to the user's memory.\\n\"\n                    f\"Relevant memories:\\n{context}\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.choices[0].message.content\n\n    # 3. Remember \u2014 save this exchange and re-index\n    save_memory(f\"## User: {user_input}\\n\\n{answer}\")\n    await mem.index()\n\n    return answer\n\n\nasync def main():\n    # Seed some knowledge\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    save_memory(\"## Decision\\nWe chose Redis for caching over Memcached.\")\n    await mem.index()\n\n    # Agent can now recall those memories\n    print(await agent_chat(\"Who is our frontend lead?\"))\n    print(await agent_chat(\"What caching solution did we pick?\"))\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#anthropic-claude-variant","title":"Anthropic Claude variant","text":"<p>Install the Anthropic extra:</p> <pre><code>$ pip install \"memsearch[anthropic]\"\n</code></pre> <p>Then swap the LLM call:</p> <pre><code>from anthropic import Anthropic\n\nllm = Anthropic()\n\n# In agent_chat(), replace the OpenAI call with:\nresp = llm.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=1024,\n    system=f\"You have these memories:\\n{context}\",\n    messages=[{\"role\": \"user\", \"content\": user_input}],\n)\nanswer = resp.content[0].text\n</code></pre>"},{"location":"getting-started/#ollama-variant-fully-local-no-api-key","title":"Ollama variant (fully local, no API key)","text":"<pre><code>$ pip install \"memsearch[ollama]\"\n$ ollama pull nomic-embed-text    # embedding model\n$ ollama pull llama3.2            # chat model\n</code></pre> <pre><code>from ollama import chat\nfrom memsearch import MemSearch\n\n# Use Ollama for embeddings too \u2014 everything stays local\nmem = MemSearch(paths=[MEMORY_DIR], embedding_provider=\"ollama\")\n\n# In agent_chat(), replace the LLM call with:\nresp = chat(\n    model=\"llama3.2\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n        {\"role\": \"user\", \"content\": user_input},\n    ],\n)\nanswer = resp.message.content\n</code></pre>"},{"location":"getting-started/#api-keys","title":"API Keys","text":"<p>Set the environment variable for your chosen embedding provider. memsearch reads standard SDK environment variables -- no custom key names.</p> Provider Env Var Notes OpenAI (default) <code>OPENAI_API_KEY</code> Included with base install OpenAI-compatible proxy <code>OPENAI_BASE_URL</code> For Azure OpenAI, vLLM, LiteLLM, etc. Google Gemini <code>GOOGLE_API_KEY</code> Requires <code>memsearch[google]</code> Voyage AI <code>VOYAGE_API_KEY</code> Requires <code>memsearch[voyage]</code> Ollama <code>OLLAMA_HOST</code> (optional) Defaults to <code>http://localhost:11434</code> Local (sentence-transformers) -- No API key needed Anthropic <code>ANTHROPIC_API_KEY</code> Used by <code>compact</code> summarization only <pre><code>$ export OPENAI_API_KEY=\"sk-...\"         # OpenAI embeddings (default)\n$ export GOOGLE_API_KEY=\"...\"            # Google Gemini embeddings\n$ export VOYAGE_API_KEY=\"...\"            # Voyage AI embeddings\n$ export ANTHROPIC_API_KEY=\"...\"         # Anthropic (for compact summarization)\n</code></pre>"},{"location":"getting-started/#milvus-backends","title":"Milvus Backends","text":"<p>memsearch works with three Milvus deployment modes. Choose based on your needs:</p> <pre><code>graph TD\n    A[memsearch] --&gt; B{Choose backend}\n    B --&gt;|\"Default&lt;br&gt;(zero config)\"| C[\"Milvus Lite&lt;br&gt;~/.memsearch/milvus.db\"]\n    B --&gt;|\"Self-hosted&lt;br&gt;(multi-agent)\"| D[\"Milvus Server&lt;br&gt;localhost:19530\"]\n    B --&gt;|\"Managed&lt;br&gt;(production)\"| E[\"Zilliz Cloud&lt;br&gt;cloud.zilliz.com\"]\n\n    style C fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style D fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style E fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1</code></pre>"},{"location":"getting-started/#milvus-lite-default-zero-config","title":"Milvus Lite (default -- zero config)","text":"<p>Data is stored in a single local <code>.db</code> file. No server to install, no ports to open.</p> <p>Best for: personal use, single-agent setups, prototyping, development.</p> PythonCLI <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"~/.memsearch/milvus.db\",  # default, can be omitted\n)\n</code></pre> <pre><code>$ memsearch index ./memory/\n# Uses ~/.memsearch/milvus.db by default\n</code></pre>"},{"location":"getting-started/#milvus-server-self-hosted","title":"Milvus Server (self-hosted)","text":"<p>Deploy Milvus via Docker or Kubernetes. Multiple agents and users can share the same server instance, each using a separate collection or database.</p> <p>Best for: team environments, multi-agent workloads, shared always-on vector store.</p> PythonCLIDocker <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"http://localhost:19530\",\n    milvus_token=\"root:Milvus\",    # default credentials\n)\n</code></pre> <pre><code>$ memsearch index ./memory/ --milvus-uri http://localhost:19530 --milvus-token root:Milvus\n</code></pre> <pre><code>$ docker run -d --name milvus \\\n    -p 19530:19530 -p 9091:9091 \\\n    milvusdb/milvus:latest milvus run standalone\n</code></pre>"},{"location":"getting-started/#zilliz-cloud-fully-managed","title":"Zilliz Cloud (fully managed)","text":"<p>Zero-ops, auto-scaling managed Milvus. Get a free cluster at cloud.zilliz.com.</p> <p>Best for: production deployments, teams that do not want to manage infrastructure.</p> PythonCLI <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"https://in03-xxx.api.gcp-us-west1.zillizcloud.com\",\n    milvus_token=\"your-api-key\",\n)\n</code></pre> <pre><code>$ memsearch index ./memory/ \\\n    --milvus-uri \"https://in03-xxx.api.gcp-us-west1.zillizcloud.com\" \\\n    --milvus-token \"your-api-key\"\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>memsearch uses a layered configuration system. Settings are resolved in priority order (lowest to highest):</p> <ol> <li>Built-in defaults -- sensible out-of-the-box values</li> <li>Global config -- <code>~/.memsearch/config.toml</code></li> <li>Project config -- <code>.memsearch.toml</code> in your working directory</li> <li>CLI flags -- <code>--milvus-uri</code>, <code>--provider</code>, etc.</li> </ol> <p>Higher-priority sources override lower ones. This means you can set defaults globally, customize per project, and override on the fly with CLI flags.</p> <p>Note: API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>) are read from environment variables by their respective SDKs. They are not stored in memsearch config files. See API Keys below.</p>"},{"location":"getting-started/#interactive-config-wizard","title":"Interactive config wizard","text":"<p>The fastest way to configure memsearch:</p> <pre><code>$ memsearch config init\nmemsearch configuration wizard\nWriting to: /home/user/.memsearch/config.toml\n\n\u2500\u2500 Milvus \u2500\u2500\n  Milvus URI [~/.memsearch/milvus.db]:\n  Milvus token (empty for none) []:\n  Collection name [memsearch_chunks]:\n\n\u2500\u2500 Embedding \u2500\u2500\n  Provider (openai/google/voyage/ollama/local) [openai]:\n  Model (empty for provider default) []:\n\n\u2500\u2500 Chunking \u2500\u2500\n  Max chunk size (chars) [1500]:\n  Overlap lines [2]:\n...\n\nConfig saved to /home/user/.memsearch/config.toml\n</code></pre> <p>Use <code>--project</code> to write to <code>.memsearch.toml</code> in the current directory instead:</p> <pre><code>$ memsearch config init --project\n</code></pre>"},{"location":"getting-started/#config-file-locations","title":"Config file locations","text":"Scope Path Use case Global <code>~/.memsearch/config.toml</code> Machine-wide defaults (Milvus URI, preferred provider) Project <code>.memsearch.toml</code> Per-project overrides (collection name, custom model) <p>Both files use TOML format:</p> <pre><code># Example ~/.memsearch/config.toml\n\n[milvus]\nuri = \"http://localhost:19530\"\ntoken = \"root:Milvus\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"\nprompt_file = \"\"\n</code></pre>"},{"location":"getting-started/#get-and-set-individual-values","title":"Get and set individual values","text":"<pre><code>$ memsearch config set milvus.uri http://localhost:19530\nSet milvus.uri = http://localhost:19530 in /home/user/.memsearch/config.toml\n\n$ memsearch config get milvus.uri\nhttp://localhost:19530\n\n$ memsearch config set embedding.provider ollama --project\nSet embedding.provider = ollama in .memsearch.toml\n</code></pre>"},{"location":"getting-started/#view-resolved-configuration","title":"View resolved configuration","text":"<pre><code>$ memsearch config list --resolved    # Final merged config from all sources\n$ memsearch config list --global      # Show ~/.memsearch/config.toml only\n$ memsearch config list --project     # Show .memsearch.toml only\n</code></pre>"},{"location":"getting-started/#cli-flag-overrides","title":"CLI flag overrides","text":"<p>CLI flags always take the highest priority:</p> <pre><code>$ memsearch index ./memory/ --provider google --milvus-uri http://localhost:19530\n$ memsearch search \"Redis config\" --top-k 10 --milvus-uri http://10.0.0.5:19530\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next","text":"<ul> <li>Architecture -- deep dive into the chunking pipeline, dedup strategy, and data flow diagrams</li> <li>CLI Reference -- complete reference for all <code>memsearch</code> commands, flags, and options</li> <li>Claude Code Plugin -- give Claude automatic persistent memory across sessions with zero configuration</li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>memsearch is a plain Python library -- it works with any framework. This page shows ready-made patterns for LangChain, LangGraph, LlamaIndex, and CrewAI.</p>"},{"location":"integrations/#langchain","title":"LangChain","text":"<pre><code>pip install langchain langchain-openai\n</code></pre>"},{"location":"integrations/#as-a-retriever","title":"As a Retriever","text":"<p>Wrap <code>MemSearch</code> in a LangChain <code>BaseRetriever</code> so it plugs into any LangChain chain or agent.</p> <pre><code>import asyncio\nfrom pydantic import ConfigDict\nfrom memsearch import MemSearch\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\n\n\nclass MemSearchRetriever(BaseRetriever):\n    \"\"\"LangChain retriever backed by memsearch.\"\"\"\n\n    mem: MemSearch\n    top_k: int = 5\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -&gt; list[Document]:\n        results = asyncio.run(self.mem.search(query, top_k=self.top_k))\n        return [\n            Document(\n                page_content=r[\"content\"],\n                metadata={\n                    \"source\": r[\"source\"],\n                    \"heading\": r[\"heading\"],\n                    \"score\": r[\"score\"],\n                },\n            )\n            for r in results\n        ]\n</code></pre> <p>Use it like any other LangChain retriever:</p> <pre><code>mem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\nretriever = MemSearchRetriever(mem=mem, top_k=3)\ndocs = retriever.invoke(\"Redis caching\")\n# [Document(page_content=\"We chose Redis for caching...\", metadata={...}), ...]\n</code></pre>"},{"location":"integrations/#rag-chain","title":"RAG Chain","text":"<p>Combine the retriever with an LLM using LCEL (LangChain Expression Language) for a simple retrieval-augmented generation pipeline:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nretriever = MemSearchRetriever(mem=mem, top_k=3)\n\n\ndef format_docs(docs: list[Document]) -&gt; str:\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"Use the following context to answer the question.\\n\\n\"\n    \"Context:\\n{context}\\n\\n\"\n    \"Question: {question}\\n\"\n    \"Answer:\"\n)\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nanswer = rag_chain.invoke(\"what caching solution are we using?\")\nprint(answer)\n</code></pre>"},{"location":"integrations/#langgraph","title":"LangGraph","text":"<pre><code>pip install langgraph langchain-openai\n</code></pre>"},{"location":"integrations/#as-a-tool-react-agent","title":"As a Tool (ReAct Agent)","text":"<p>Wrap memsearch as a tool and let a LangGraph ReAct agent decide when to search:</p> <pre><code>import asyncio\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom memsearch import MemSearch\n\nmem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\n\n@tool\ndef search_memory(query: str) -&gt; str:\n    \"\"\"Search the team's knowledge base for relevant information.\"\"\"\n    results = asyncio.run(mem.search(query, top_k=3))\n    if not results:\n        return \"No relevant memories found.\"\n    return \"\\n\\n\".join(\n        f\"[{r['source']}] {r['heading']}: {r['content'][:300]}\"\n        for r in results\n    )\n\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nagent = create_react_agent(llm, [search_memory])\n\nresult = agent.invoke(\n    {\"messages\": [(\"user\", \"Who is our frontend lead and what did they work on?\")]}\n)\n\n# The agent automatically calls search_memory when it needs information\nfor msg in result[\"messages\"]:\n    role = msg.__class__.__name__\n    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n        print(f\"{role}: [called search_memory]\")\n    elif hasattr(msg, \"content\") and msg.content:\n        print(f\"{role}: {msg.content[:200]}\")\n</code></pre> <p>The agent will autonomously decide when to call <code>search_memory</code> based on the user's question -- no manual retrieval logic needed.</p>"},{"location":"integrations/#llamaindex","title":"LlamaIndex","text":"<pre><code>pip install llama-index-core\n</code></pre>"},{"location":"integrations/#as-a-retriever_1","title":"As a Retriever","text":"<p>Implement a LlamaIndex <code>BaseRetriever</code> that delegates to memsearch. Results are returned as <code>NodeWithScore</code> objects that work with any LlamaIndex query engine or pipeline.</p> <pre><code>import asyncio\nfrom typing import List\nfrom memsearch import MemSearch\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core.schema import NodeWithScore, TextNode, QueryBundle\n\n\nclass MemSearchRetriever(BaseRetriever):\n    \"\"\"LlamaIndex retriever backed by memsearch.\"\"\"\n\n    def __init__(self, mem: MemSearch, top_k: int = 5) -&gt; None:\n        self._mem = mem\n        self._top_k = top_k\n        super().__init__()\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        results = asyncio.run(\n            self._mem.search(query_bundle.query_str, top_k=self._top_k)\n        )\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=r[\"content\"],\n                    metadata={\"source\": r[\"source\"], \"heading\": r[\"heading\"]},\n                ),\n                score=r[\"score\"],\n            )\n            for r in results\n        ]\n</code></pre> <p>Use it like any other LlamaIndex retriever:</p> <pre><code>mem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\nretriever = MemSearchRetriever(mem=mem, top_k=3)\nnodes = retriever.retrieve(\"Redis caching\")\nfor n in nodes:\n    print(f\"[{n.score:.4f}] {n.node.metadata['source']} \u2014 {n.node.text[:100]}\")\n</code></pre> <p>Plug it into a <code>RetrieverQueryEngine</code> for end-to-end RAG (requires an LLM provider like <code>llama-index-llms-openai</code>):</p> <pre><code>from llama_index.core.query_engine import RetrieverQueryEngine\n\nquery_engine = RetrieverQueryEngine.from_args(retriever)\nresponse = query_engine.query(\"what caching solution are we using?\")\nprint(response)\n</code></pre>"},{"location":"integrations/#crewai","title":"CrewAI","text":"<pre><code>pip install crewai\n</code></pre>"},{"location":"integrations/#as-a-tool-multi-agent-crew","title":"As a Tool (Multi-Agent Crew)","text":"<p>Register memsearch as a CrewAI tool so any agent in the crew can search the knowledge base:</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\nfrom crewai import Agent, Task, Crew\nfrom crewai.tools import tool\n\nmem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\n\n@tool(\"search_memory\")\ndef search_memory(query: str) -&gt; str:\n    \"\"\"Search the team's knowledge base for relevant information.\"\"\"\n    results = asyncio.run(mem.search(query, top_k=3))\n    if not results:\n        return \"No relevant memories found.\"\n    return \"\\n\\n\".join(\n        f\"[{r['source']}] {r['heading']}: {r['content'][:300]}\"\n        for r in results\n    )\n\n\nresearcher = Agent(\n    role=\"Knowledge Base Researcher\",\n    goal=\"Find relevant information from the team's knowledge base\",\n    backstory=\"You are a researcher who searches the team's knowledge base to answer questions.\",\n    tools=[search_memory],\n)\n\nresearch_task = Task(\n    description=\"Who is the frontend lead and what did they work on recently?\",\n    expected_output=\"A short summary mentioning the frontend lead's name and recent work.\",\n    agent=researcher,\n)\n\ncrew = Crew(agents=[researcher], tasks=[research_task])\nresult = crew.kickoff()\nprint(result)\n</code></pre> <p>The agent will automatically call <code>search_memory</code> to look up the answer before responding.</p>"},{"location":"python-api/","title":"Python API","text":"<p>memsearch provides a high-level Python API through the <code>MemSearch</code> class. Import it, point it at your markdown files, and you get semantic memory for your agent in a few lines of code.</p> <pre><code>from memsearch import MemSearch\n\nmem = MemSearch(paths=[\"./memory\"])\n\nawait mem.index()                                      # index markdown files\nresults = await mem.search(\"Redis config\", top_k=3)    # semantic search\nprint(results[0][\"content\"], results[0][\"score\"])       # content + similarity\n</code></pre>"},{"location":"python-api/#memsearch","title":"<code>MemSearch</code>","text":"<p>The main entry point. Handles indexing, search, compaction, and file watching.</p>"},{"location":"python-api/#constructor","title":"Constructor","text":"<pre><code>MemSearch(\n    paths=[\"./memory\"],\n    *,\n    embedding_provider=\"openai\",\n    embedding_model=None,\n    milvus_uri=\"~/.memsearch/milvus.db\",\n    milvus_token=None,\n    collection=\"memsearch_chunks\",\n    max_chunk_size=1500,\n    overlap_lines=2,\n)\n</code></pre> Parameter Type Default Description <code>paths</code> <code>list[str \\| Path]</code> <code>[]</code> Directories or files to index <code>embedding_provider</code> <code>str</code> <code>\"openai\"</code> Embedding backend (<code>\"openai\"</code>, <code>\"google\"</code>, <code>\"voyage\"</code>, <code>\"ollama\"</code>, <code>\"local\"</code>) <code>embedding_model</code> <code>str \\| None</code> <code>None</code> Override the default model for the chosen provider <code>milvus_uri</code> <code>str</code> <code>\"~/.memsearch/milvus.db\"</code> Milvus connection URI \u2014 local <code>.db</code> path for Milvus Lite, <code>http://host:port</code> for Milvus Server, or <code>https://*.zillizcloud.com</code> for Zilliz Cloud <code>milvus_token</code> <code>str \\| None</code> <code>None</code> Auth token for Milvus Server or Zilliz Cloud <code>collection</code> <code>str</code> <code>\"memsearch_chunks\"</code> Milvus collection name. Use different names to isolate agents sharing the same backend <code>max_chunk_size</code> <code>int</code> <code>1500</code> Maximum chunk size in characters <code>overlap_lines</code> <code>int</code> <code>2</code> Overlapping lines between adjacent chunks"},{"location":"python-api/#context-manager","title":"Context Manager","text":"<p><code>MemSearch</code> implements the context manager protocol. Use <code>with</code> to ensure resources are released:</p> <pre><code>with MemSearch(paths=[\"./memory\"]) as mem:\n    await mem.index()\n    results = await mem.search(\"Redis config\")\n# Milvus connection is closed automatically\n</code></pre> <p>Or call <code>mem.close()</code> manually when done.</p>"},{"location":"python-api/#methods","title":"Methods","text":""},{"location":"python-api/#index","title":"<code>index</code>","text":"<pre><code>await mem.index(*, force=False) -&gt; int\n</code></pre> <p>Scan all configured paths and index every markdown file (<code>.md</code>, <code>.markdown</code>) into the vector store. Returns the number of chunks indexed.</p> Parameter Type Default Description <code>force</code> <code>bool</code> <code>False</code> Re-embed all chunks even if unchanged. Use this after switching embedding providers <p>Behavior:</p> <ul> <li>Incremental by default. Only new or changed chunks are embedded. Unchanged chunks are skipped via content-hash dedup.</li> <li>Stale cleanup. Chunks from deleted files are automatically removed.</li> <li>Deleted content. If a section is removed from a file, its old chunks are cleaned up on the next <code>index()</code> call.</li> </ul> <pre><code>mem = MemSearch(paths=[\"./memory\", \"./notes\"])\nn = await mem.index()\nprint(f\"Indexed {n} chunks\")\n\n# After switching to a different embedding provider, force re-index\nn = await mem.index(force=True)\n</code></pre>"},{"location":"python-api/#index_file","title":"<code>index_file</code>","text":"<pre><code>await mem.index_file(path) -&gt; int\n</code></pre> <p>Index a single file. Returns the number of chunks indexed.</p> Parameter Type Description <code>path</code> <code>str \\| Path</code> Path to a markdown file <pre><code>n = await mem.index_file(\"./memory/2026-02-12.md\")\n</code></pre>"},{"location":"python-api/#search","title":"<code>search</code>","text":"<pre><code>await mem.search(query, *, top_k=10) -&gt; list[dict]\n</code></pre> <p>Semantic search across indexed chunks. Returns a list of result dicts, sorted by relevance.</p> Parameter Type Default Description <code>query</code> <code>str</code> (required) Natural-language search query <code>top_k</code> <code>int</code> <code>10</code> Maximum number of results <p>Return value: Each dict contains:</p> Key Type Description <code>content</code> <code>str</code> The chunk text <code>source</code> <code>str</code> Path to the source markdown file <code>heading</code> <code>str</code> The heading this chunk belongs to <code>heading_level</code> <code>int</code> Heading level (1\u20136, or 0 for no heading) <code>chunk_hash</code> <code>str</code> Unique chunk identifier <code>start_line</code> <code>int</code> Start line in the source file <code>end_line</code> <code>int</code> End line in the source file <code>score</code> <code>float</code> Relevance score (higher is better) <pre><code>results = await mem.search(\"who is the frontend lead?\", top_k=5)\nfor r in results:\n    print(f\"[{r['score']:.4f}] {r['heading']}: {r['content'][:100]}\")\n</code></pre>"},{"location":"python-api/#compact","title":"<code>compact</code>","text":"<pre><code>await mem.compact(\n    *,\n    source=None,\n    llm_provider=\"openai\",\n    llm_model=None,\n    prompt_template=None,\n    output_dir=None,\n) -&gt; str\n</code></pre> <p>Use an LLM to compress indexed chunks into a summary. The summary is appended to <code>memory/YYYY-MM-DD.md</code> and automatically indexed.</p> Parameter Type Default Description <code>source</code> <code>str \\| None</code> <code>None</code> Only compact chunks from this source file. <code>None</code> = all chunks <code>llm_provider</code> <code>str</code> <code>\"openai\"</code> LLM backend (<code>\"openai\"</code>, <code>\"anthropic\"</code>, <code>\"gemini\"</code>) <code>llm_model</code> <code>str \\| None</code> <code>None</code> Override the default LLM model <code>prompt_template</code> <code>str \\| None</code> <code>None</code> Custom prompt (must contain <code>{chunks}</code> placeholder) <code>output_dir</code> <code>str \\| Path \\| None</code> <code>None</code> Where to write the summary. Defaults to the first configured path <p>Default LLM models:</p> Provider Default Model <code>openai</code> <code>gpt-4o-mini</code> <code>anthropic</code> <code>claude-sonnet-4-5-20250929</code> <code>gemini</code> <code>gemini-2.0-flash</code> <pre><code># Compact all memories\nsummary = await mem.compact()\nprint(summary)\n\n# Compact only one file, using Claude\nsummary = await mem.compact(\n    source=\"./memory/old-notes.md\",\n    llm_provider=\"anthropic\",\n)\n</code></pre>"},{"location":"python-api/#watch","title":"<code>watch</code>","text":"<pre><code>mem.watch(*, on_event=None, debounce_ms=None) -&gt; FileWatcher\n</code></pre> <p>Start a background file watcher that auto-indexes markdown changes. This is a synchronous method that returns a <code>FileWatcher</code> object running in a background thread.</p> Parameter Type Default Description <code>on_event</code> <code>Callable</code> <code>None</code> Callback invoked after each event: <code>(event_type, summary, file_path)</code>. <code>event_type</code> is <code>\"created\"</code>, <code>\"modified\"</code>, or <code>\"deleted\"</code> <code>debounce_ms</code> <code>int \\| None</code> <code>None</code> Debounce delay in milliseconds. Defaults to 1500 if not set <p>Returns: a <code>FileWatcher</code> instance. Call <code>watcher.stop()</code> to stop watching, or use it as a context manager.</p> <pre><code>mem = MemSearch(paths=[\"./memory\"])\nawait mem.index()  # initial index\n\n# Start watching for changes in the background\nwatcher = mem.watch(on_event=lambda t, s, p: print(f\"[{t}] {s}\"))\n\n# ... your agent runs here ...\n\nwatcher.stop()\n</code></pre>"},{"location":"python-api/#close","title":"<code>close</code>","text":"<pre><code>mem.close() -&gt; None\n</code></pre> <p>Release the Milvus connection and other resources. Called automatically when using <code>MemSearch</code> as a context manager.</p>"},{"location":"python-api/#full-example","title":"Full Example","text":"<p>A complete agent loop: seed knowledge, index it, then recall it during conversation.</p> OpenAIAnthropic ClaudeOllama (fully local) <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = OpenAI()\nmem = MemSearch(paths=[MEMORY_DIR])\n\ndef save_memory(content: str):\n    \"\"\"Append a note to today's memory log.\"\"\"\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    # 1. Recall \u2014 search past memories\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    # 2. Think \u2014 call LLM with memory context\n    resp = llm.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.choices[0].message.content\n\n    # 3. Remember \u2014 save and index\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    save_memory(\"## Decision\\nWe chose Redis for caching over Memcached.\")\n    await mem.index()  # or mem.watch() to auto-index in the background\n\n    print(await agent_chat(\"Who is our frontend lead?\"))\n    print(await agent_chat(\"What caching solution did we pick?\"))\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom anthropic import Anthropic\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = Anthropic()\nmem = MemSearch(paths=[MEMORY_DIR])\n\ndef save_memory(content: str):\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    resp = llm.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=1024,\n        system=f\"You have these memories:\\n{context}\",\n        messages=[{\"role\": \"user\", \"content\": user_input}],\n    )\n    answer = resp.content[0].text\n\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    await mem.index()\n    print(await agent_chat(\"Who is our frontend lead?\"))\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom ollama import chat\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nmem = MemSearch(paths=[MEMORY_DIR], embedding_provider=\"ollama\")\n\ndef save_memory(content: str):\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    resp = chat(\n        model=\"llama3.2\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.message.content\n\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    await mem.index()\n    print(await agent_chat(\"Who is our frontend lead?\"))\n\nasyncio.run(main())\n</code></pre>"}]}